{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Apache Iceberg for Telecom Enterprises (Google Colab)\n",
        "## A Complete Guide with PySpark & Telecom Time Series Data\n",
        "\n",
        "This notebook demonstrates how to use Apache Iceberg in telecom enterprise environments using PySpark on **Google Colab**.\n",
        "\n",
        "### Key Features:\n",
        "- **ACID transactions** for reliable telecom data operations\n",
        "- **Schema evolution** for adapting to new network technologies  \n",
        "- **Time travel** for historical network performance analysis\n",
        "- **Hidden partitioning** for optimal time-series data queries\n",
        "- **Data compaction** for efficient storage of large telecom datasets\n",
        "\n",
        "### Google Colab Advantages:\n",
        "- Pre-configured Python environment\n",
        "- Free GPU/TPU access\n",
        "- No local setup required\n",
        "- Easy sharing and collaboration\n",
        "\n",
        "**Note:** This notebook is optimized specifically for Google Colab environment.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Google Colab Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"üî¨ Google Colab Apache Iceberg Setup\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check if running in Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"‚úÖ Running in Google Colab\")\n",
        "    \n",
        "    # Install Java (usually pre-installed in Colab)\n",
        "    print(\"‚òï Setting up Java for Colab...\")\n",
        "    !apt-get update -qq\n",
        "    !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null 2>&1\n",
        "    \n",
        "    # Set JAVA_HOME for Colab\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    print(f\"üè† JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå This notebook is designed for Google Colab\")\n",
        "    print(\"üí° Please run this in Google Colab for best results\")\n",
        "    \n",
        "print(\"‚úÖ Java setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python packages optimized for Google Colab\n",
        "print(\"üì¶ Installing packages for Google Colab...\")\n",
        "\n",
        "# Use Colab-compatible installation approach\n",
        "%pip install -q pyspark>=3.4.0\n",
        "%pip install -q pyiceberg --no-deps  # No deps to avoid conflicts with Colab packages\n",
        "%pip install -q s3fs>=2023.1.0  # For S3 support\n",
        "\n",
        "print(\"‚úÖ Package installation completed!\")\n",
        "\n",
        "# Test imports\n",
        "try:\n",
        "    import pyspark\n",
        "    print(f\"‚ö° PySpark: {pyspark.__version__}\")\n",
        "    \n",
        "    # Use existing pandas/numpy from Colab\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    print(f\"üìä Pandas: {pd.__version__}\")\n",
        "    print(f\"üî¢ NumPy: {np.__version__}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"üí° Try restarting runtime if errors persist\")\n",
        "    \n",
        "print(\"üöÄ Colab setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spark + Iceberg Setup for Google Colab\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, avg, count\n",
        "import os\n",
        "\n",
        "# Colab-specific paths\n",
        "jar_url = \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar\"\n",
        "jar_path = \"/content/iceberg-spark-runtime.jar\"\n",
        "warehouse_path = \"/content/iceberg-warehouse\"\n",
        "\n",
        "print(\"üì• Downloading Iceberg JAR for Colab...\")\n",
        "!wget -q {jar_url} -O {jar_path}\n",
        "\n",
        "print(\"‚ö° Initializing Spark with Iceberg in Colab...\")\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Iceberg Telecom Demo - Colab\") \\\n",
        "    .config(\"spark.jars\", jar_path) \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .config(\"spark.sql.catalog.local.warehouse\", warehouse_path) \\\n",
        "    .config(\"spark.sql.warehouse.dir\", warehouse_path) \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"‚úÖ Spark {spark.version} with Iceberg ready in Colab!\")\n",
        "print(f\"üìÅ Warehouse: {warehouse_path}\")\n",
        "\n",
        "# Generate Telecom Data for Colab\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "print(\"\\nüì° Generating Telecom Data for Colab...\")\n",
        "\n",
        "# Colab-optimized configuration (smaller dataset)\n",
        "chunk_size_seconds = 30\n",
        "site_count = 50  # Reduced for Colab\n",
        "start_time = datetime(2023, 1, 1)\n",
        "demo_chunks = 20  # Reduced for Colab\n",
        "\n",
        "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
        "cities = {\n",
        "    \"North\": [\"Dendam\", \"Rondburg\"],\n",
        "    \"South\": [\"Schieveste\"],\n",
        "    \"East\": [\"Schipstad\", \"Dort\"],\n",
        "    \"West\": [\"Damstad\"],\n",
        "}\n",
        "technologies = [\"6G\", \"7G\"]\n",
        "vendors = [\"Ericia\", \"Noson\", \"Weihu\"]\n",
        "\n",
        "# Generate site metadata\n",
        "sites = []\n",
        "for i in range(site_count):\n",
        "    region = random.choice(regions)\n",
        "    city = random.choice(cities[region])\n",
        "    tech = random.choices(technologies, weights=[0.5, 0.5])[0]\n",
        "    vendor = random.choices(vendors, weights=[0.4, 0.4, 0.2])[0]\n",
        "    site_id = f\"SITE_{i:05d}\"\n",
        "    sites.append({\n",
        "        \"site_id\": site_id,\n",
        "        \"region\": region,\n",
        "        \"city\": city,\n",
        "        \"technology\": tech,\n",
        "        \"vendor\": vendor,\n",
        "    })\n",
        "\n",
        "def generate_telecom_data_chunk(second_offset):\n",
        "    timestamp = start_time + timedelta(seconds=second_offset)\n",
        "    records = []\n",
        "    for site in sites:\n",
        "        # Simplified telecom metrics generation for Colab\n",
        "        base_rssi = -75 if site[\"technology\"] == \"7G\" else -85\n",
        "        rssi = np.random.normal(loc=base_rssi, scale=3)\n",
        "        \n",
        "        base_latency = 35 if site[\"technology\"] == \"7G\" else 60\n",
        "        latency = np.random.normal(loc=base_latency, scale=8)\n",
        "        \n",
        "        data_volume = np.random.exponential(scale=6)\n",
        "        cpu_usage = np.clip(np.random.normal(loc=50, scale=10), 0, 100)\n",
        "        drop_rate = np.random.beta(1, 200) * 100\n",
        "\n",
        "        records.append({\n",
        "            \"timestamp\": timestamp,\n",
        "            \"region\": site[\"region\"],\n",
        "            \"city\": site[\"city\"],\n",
        "            \"site_id\": site[\"site_id\"],\n",
        "            \"technology\": site[\"technology\"],\n",
        "            \"vendor\": site[\"vendor\"],\n",
        "            \"rssi_dbm\": round(rssi, 2),\n",
        "            \"latency_ms\": round(latency, 2),\n",
        "            \"data_volume_mb\": round(data_volume, 2),\n",
        "            \"drop_rate_percent\": round(drop_rate, 2),\n",
        "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
        "        })\n",
        "    return records\n",
        "\n",
        "# Generate data\n",
        "all_records = []\n",
        "for chunk_idx in range(demo_chunks):\n",
        "    for second in range(chunk_size_seconds):\n",
        "        minute_offset = chunk_idx * chunk_size_seconds + second\n",
        "        all_records.extend(generate_telecom_data_chunk(minute_offset))\n",
        "\n",
        "# Create DataFrames\n",
        "telecom_metrics_df = spark.createDataFrame(all_records)\n",
        "sites_df = spark.createDataFrame(sites)\n",
        "\n",
        "print(f\"‚úÖ Colab telecom data generated!\")\n",
        "print(f\"   üì° Sites: {sites_df.count():,}\")\n",
        "print(f\"   üìä Metrics: {telecom_metrics_df.count():,}\")\n",
        "\n",
        "telecom_metrics_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Iceberg Tables in Google Colab\n",
        "print(\"üèóÔ∏è Creating Iceberg tables in Colab...\")\n",
        "\n",
        "# Create telecom sites table\n",
        "sites_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"local.db.telecom_sites\")\n",
        "\n",
        "# Create telecom metrics table with timestamp partitioning\n",
        "telecom_metrics_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"timestamp\") \\\n",
        "    .saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"‚úÖ Iceberg tables created in Colab!\")\n",
        "\n",
        "# Verify tables\n",
        "spark.sql(\"SHOW TABLES IN local.db\").show()\n",
        "\n",
        "# Quick analytics optimized for Colab\n",
        "print(\"\\nüìä Colab Telecom Analytics:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.vendor,\n",
        "        COUNT(DISTINCT s.site_id) as sites,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
        "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.vendor\n",
        "    ORDER BY avg_rssi DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# Time travel demo for Colab\n",
        "print(\"\\nüïê Time Travel Demo:\")\n",
        "snapshots = spark.sql(\"SELECT snapshot_id, committed_at FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\").collect()\n",
        "print(f\"üì∏ Available snapshots: {len(snapshots)}\")\n",
        "\n",
        "for i, snapshot in enumerate(snapshots):\n",
        "    print(f\"   Snapshot {i+1}: {snapshot['snapshot_id']} at {snapshot['committed_at']}\")\n",
        "\n",
        "print(\"\\n‚úÖ Google Colab Iceberg Demo Completed!\")\n",
        "print(\"üîó Share this notebook with your team for collaboration!\")\n",
        "\n",
        "# Optional: Stop Spark to free memory\n",
        "# spark.stop()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Basic Iceberg Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Iceberg Operations - Colab Optimized\n",
        "print(\"üîß Basic Iceberg Operations (Colab)\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Table Information\n",
        "print(\"\\nüìã Table Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.telecom_metrics\").show()\n",
        "\n",
        "# 2. Quick data exploration\n",
        "print(\"\\nüìä Data Overview:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(DISTINCT site_id) as unique_sites,\n",
        "        MIN(timestamp) as earliest_record,\n",
        "        MAX(timestamp) as latest_record\n",
        "    FROM local.db.telecom_metrics\n",
        "\"\"\").show()\n",
        "\n",
        "# 3. Performance by vendor (Colab-friendly query)\n",
        "print(\"\\nüè¢ Vendor Performance Summary:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.vendor,\n",
        "        COUNT(DISTINCT s.site_id) as sites,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.vendor\n",
        "    ORDER BY avg_rssi DESC\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"‚úÖ Basic operations completed in Colab!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Time Travel & Schema Evolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Travel & Schema Evolution - Colab Demo\n",
        "print(\"üïê Time Travel & Schema Evolution (Colab)\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# 1. Show current snapshots\n",
        "print(\"\\nüì∏ Current Snapshots:\")\n",
        "snapshots = spark.sql(\"SELECT snapshot_id, committed_at FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\")\n",
        "snapshots.show()\n",
        "\n",
        "# 2. Add new column for Colab demo\n",
        "print(\"\\nüîÑ Schema Evolution Demo:\")\n",
        "spark.sql(\"ALTER TABLE local.db.telecom_metrics ADD COLUMN signal_strength_category STRING\").collect()\n",
        "print(\"   ‚úÖ Added signal_strength_category column\")\n",
        "\n",
        "# 3. Insert data with new schema\n",
        "print(\"\\nüìä Adding categorized data...\")\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Create sample data with categories\n",
        "new_colab_records = []\n",
        "for site in sites[:10]:  # Small dataset for Colab\n",
        "    rssi = np.random.normal(loc=-75, scale=5)\n",
        "    category = \"Excellent\" if rssi > -70 else (\"Good\" if rssi > -80 else \"Poor\")\n",
        "    \n",
        "    new_colab_records.append({\n",
        "        \"timestamp\": datetime.now(),\n",
        "        \"region\": site[\"region\"],\n",
        "        \"city\": site[\"city\"],\n",
        "        \"site_id\": site[\"site_id\"],\n",
        "        \"technology\": site[\"technology\"],\n",
        "        \"vendor\": site[\"vendor\"],\n",
        "        \"rssi_dbm\": round(rssi, 2),\n",
        "        \"latency_ms\": round(np.random.normal(loc=45, scale=10), 2),\n",
        "        \"data_volume_mb\": round(np.random.exponential(scale=5), 2),\n",
        "        \"drop_rate_percent\": round(np.random.beta(1, 200) * 100, 2),\n",
        "        \"cpu_usage_percent\": round(np.clip(np.random.normal(loc=50, scale=10), 0, 100), 2),\n",
        "        \"signal_strength_category\": category\n",
        "    })\n",
        "\n",
        "new_colab_df = spark.createDataFrame(new_colab_records)\n",
        "new_colab_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"‚úÖ Categorized data added!\")\n",
        "\n",
        "# 4. Query with new schema\n",
        "print(\"\\nüìä Signal Strength Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        signal_strength_category,\n",
        "        COUNT(*) as count,\n",
        "        ROUND(AVG(rssi_dbm), 2) as avg_rssi\n",
        "    FROM local.db.telecom_metrics \n",
        "    WHERE signal_strength_category IS NOT NULL\n",
        "    GROUP BY signal_strength_category\n",
        "    ORDER BY avg_rssi DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# 5. Show updated snapshots\n",
        "print(\"\\nüì∏ Updated Snapshots:\")\n",
        "updated_snapshots = spark.sql(\"SELECT snapshot_id, committed_at FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\")\n",
        "updated_snapshots.show()\n",
        "\n",
        "print(\"‚úÖ Time travel & schema evolution demo completed in Colab!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Colab Analytics & Best Practices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab Analytics & Best Practices\n",
        "print(\"üöÄ Colab Analytics & Best Practices\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Colab-optimized analytics\n",
        "print(\"\\nüìä Telecom Network Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.region,\n",
        "        s.technology,\n",
        "        COUNT(*) as measurements,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_signal,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
        "        CASE \n",
        "            WHEN AVG(m.rssi_dbm) > -75 THEN 'Good Coverage'\n",
        "            ELSE 'Coverage Issues'\n",
        "        END as coverage_status\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.region, s.technology\n",
        "    ORDER BY avg_signal DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# 2. Simple visualization data prep\n",
        "print(\"\\nüìà Visualization Data (for plotting in Colab):\")\n",
        "viz_data = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.vendor,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.vendor\n",
        "    ORDER BY s.vendor\n",
        "\"\"\")\n",
        "\n",
        "# Convert to Pandas for easy plotting in Colab\n",
        "viz_df = viz_data.toPandas()\n",
        "print(\"üìä Data ready for matplotlib/seaborn plotting:\")\n",
        "print(viz_df)\n",
        "\n",
        "# 3. Colab Best Practices\n",
        "print(\"\\nüìö Google Colab Best Practices:\")\n",
        "print(\"\"\"\n",
        "üî¨ COLAB OPTIMIZATION:\n",
        "   ‚Ä¢ Use smaller datasets for faster execution\n",
        "   ‚Ä¢ Leverage Colab's free GPU/TPU when available\n",
        "   ‚Ä¢ Save intermediate results to avoid re-computation\n",
        "   ‚Ä¢ Use %%time magic commands to measure performance\n",
        "\n",
        "üìä DATA VISUALIZATION:\n",
        "   ‚Ä¢ Convert Spark DataFrames to Pandas for plotting\n",
        "   ‚Ä¢ Use matplotlib, seaborn, or plotly for visualizations\n",
        "   ‚Ä¢ Create interactive plots with widgets\n",
        "   ‚Ä¢ Export plots to Google Drive\n",
        "\n",
        "ü§ù COLLABORATION:\n",
        "   ‚Ä¢ Share notebooks via Google Drive links\n",
        "   ‚Ä¢ Use comments and markdown for documentation\n",
        "   ‚Ä¢ Version control with Git integration\n",
        "   ‚Ä¢ Export to GitHub for team collaboration\n",
        "\n",
        "üíæ DATA PERSISTENCE:\n",
        "   ‚Ä¢ Mount Google Drive for data persistence\n",
        "   ‚Ä¢ Use Colab's built-in file system for temporary storage\n",
        "   ‚Ä¢ Export results to CSV/Parquet for later use\n",
        "   ‚Ä¢ Consider upgrading to Colab Pro for more resources\n",
        "\"\"\")\n",
        "\n",
        "# 4. Final statistics\n",
        "print(\"\\nüìà Final Colab Demo Statistics:\")\n",
        "total_records = spark.sql(\"SELECT COUNT(*) as total FROM local.db.telecom_metrics\").collect()[0]['total']\n",
        "total_snapshots = spark.sql(\"SELECT COUNT(*) as snapshots FROM local.db.telecom_metrics.snapshots\").collect()[0]['snapshots']\n",
        "\n",
        "print(f\"   üìä Total Records: {total_records:,}\")\n",
        "print(f\"   üì∏ Total Snapshots: {total_snapshots}\")\n",
        "print(f\"   üî¨ Platform: Google Colab\")\n",
        "print(f\"   ‚ö° Spark Version: {spark.version}\")\n",
        "\n",
        "print(\"\\nüéâ Google Colab Iceberg Demo Complete!\")\n",
        "print(\"üîó Share this notebook with your team for collaborative learning!\")\n",
        "\n",
        "# Optional: Create a simple plot if matplotlib is available\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(viz_df['vendor'], viz_df['avg_rssi'])\n",
        "    plt.title('Average RSSI by Vendor')\n",
        "    plt.ylabel('RSSI (dBm)')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(viz_df['vendor'], viz_df['avg_latency'])\n",
        "    plt.title('Average Latency by Vendor')\n",
        "    plt.ylabel('Latency (ms)')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üìä Visualization created successfully!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"üìä Install matplotlib for visualizations: !pip install matplotlib\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
