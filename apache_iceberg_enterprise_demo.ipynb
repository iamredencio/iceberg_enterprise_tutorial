{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Apache Iceberg for Telecom Enterprises\n",
    "## A Complete Guide with PySpark & Telecom Time Series Data\n",
    "\n",
    "This notebook demonstrates how to use Apache Iceberg in telecom enterprise environments using PySpark. Apache Iceberg is an open table format for huge analytic datasets that provides:\n",
    "\n",
    "- **ACID transactions** for reliable telecom data operations\n",
    "- **Schema evolution** for adapting to new network technologies\n",
    "- **Time travel** for historical network performance analysis\n",
    "- **Hidden partitioning** for optimal time-series data queries\n",
    "- **Data compaction** for efficient storage of large telecom datasets\n",
    "- **Rollback capabilities** for data recovery scenarios\n",
    "\n",
    "### Why Iceberg for Telecom Enterprises?\n",
    "- Handle massive time-series data from network infrastructure\n",
    "- Multi-engine compatibility (Spark, Flink, Trino, etc.) for different analytics needs\n",
    "- Better performance for network monitoring and analytics\n",
    "- Enterprise-grade data governance for regulatory compliance\n",
    "- Support for real-time and batch processing of telecom metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's set up the required dependencies. This notebook handles both Google Colab and local environments automatically and will generate synthetic telecom time-series data for demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "system = platform.system()\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "print(f\"üñ•Ô∏è OS: {system}\")\n",
    "print(f\"üìî Environment: {'Google Colab' if in_colab else 'Local Jupyter'}\")\n",
    "\n",
    "# Install Java (required for PySpark)\n",
    "if system == \"Linux\" and not in_colab:\n",
    "    # Linux environment (not Colab)\n",
    "    print(\"üì¶ Installing Java on Linux...\")\n",
    "    !apt-get update -qq\n",
    "    !apt-get install -y openjdk-8-jdk-headless -qq\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "elif system == \"Darwin\":\n",
    "    # macOS environment\n",
    "    print(\"üì¶ Setting up Java on macOS...\")\n",
    "    try:\n",
    "        java_version = subprocess.run(['java', '-version'], capture_output=True, text=True, stderr=subprocess.STDOUT)\n",
    "        if java_version.returncode == 0:\n",
    "            print(\"‚òï Java is already installed\")\n",
    "            # Try to find JAVA_HOME\n",
    "            try:\n",
    "                java_home = subprocess.run(['/usr/libexec/java_home'], capture_output=True, text=True)\n",
    "                if java_home.returncode == 0:\n",
    "                    os.environ[\"JAVA_HOME\"] = java_home.stdout.strip()\n",
    "                    print(f\"üè† JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Could not automatically set JAVA_HOME\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Java not found. Please install Java 8+ using:\")\n",
    "            print(\"   brew install openjdk@8\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Java not found. Please install Java 8+ using:\")\n",
    "        print(\"   brew install openjdk@8\")\n",
    "elif in_colab:\n",
    "    # Google Colab - Java is pre-installed\n",
    "    print(\"‚òï Using pre-installed Java in Colab\")\n",
    "    # Try to set JAVA_HOME for Colab\n",
    "    possible_java_homes = [\n",
    "        \"/usr/lib/jvm/java-11-openjdk-amd64\",\n",
    "        \"/usr/lib/jvm/java-8-openjdk-amd64\",\n",
    "        \"/usr/lib/jvm/default-java\"\n",
    "    ]\n",
    "    for java_home in possible_java_homes:\n",
    "        if os.path.exists(java_home):\n",
    "            os.environ[\"JAVA_HOME\"] = java_home\n",
    "            print(f\"üè† JAVA_HOME set to: {java_home}\")\n",
    "            break\n",
    "\n",
    "print(f\"‚òï JAVA_HOME: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "print(\"‚úÖ Java setup completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages with dependency management\n",
    "print(\"üì¶ Installing Python packages...\")\n",
    "\n",
    "# For Google Colab, use compatible versions\n",
    "if in_colab:\n",
    "    print(\"üîß Installing Colab-compatible versions...\")\n",
    "    # Use existing pandas and numpy versions in Colab to avoid conflicts\n",
    "    %pip install -q pyspark>=3.4.0\n",
    "    %pip install -q pyiceberg --no-deps  # Install without dependencies to avoid conflicts\n",
    "    %pip install -q s3fs  # For S3 support\n",
    "else:\n",
    "    # For local environments, install specific versions\n",
    "    print(\"üîß Installing packages for local environment...\")\n",
    "    %pip install -q pyspark==3.4.1\n",
    "    %pip install -q pyiceberg[s3fs]==0.5.1\n",
    "    %pip install -q pandas>=2.0.0\n",
    "    %pip install -q matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Package installation completed!\")\n",
    "\n",
    "# Import required libraries\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Pandas import failed: {e}\")\n",
    "    print(\"üí° Try restarting the runtime and running cells again\")\n",
    "\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"‚ö° PySpark version: {pyspark.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è PySpark import failed: {e}\")\n",
    "    \n",
    "print(\"üöÄ Ready to proceed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Spark Configuration with Iceberg\n",
    "\n",
    "Configure Spark to work with Apache Iceberg. In enterprise environments, you would typically configure this in your cluster settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# Download Iceberg JAR for Spark\n",
    "jar_url = \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar\"\n",
    "\n",
    "if in_colab:\n",
    "    jar_path = \"/content/iceberg-spark-runtime.jar\"\n",
    "    warehouse_path = \"/content/iceberg-warehouse\"\n",
    "else:\n",
    "    jar_path = \"./iceberg-spark-runtime.jar\"\n",
    "    warehouse_path = \"./iceberg-warehouse\"\n",
    "\n",
    "print(f\"üì• Downloading Iceberg JAR to {jar_path}...\")\n",
    "!wget -q {jar_url} -O {jar_path}\n",
    "\n",
    "# Configure Spark with Iceberg\n",
    "print(\"‚ö° Initializing Spark with Iceberg...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Enterprise Demo\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", warehouse_path) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_path) \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark {spark.version} with Iceberg initialized successfully!\")\n",
    "print(f\"üìÅ Warehouse location: {warehouse_path}\")\n",
    "print(f\"‚òï Java Home: {os.environ.get('JAVA_HOME', 'Not set')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Generating Synthetic Telecom Time Series Data\n",
    "\n",
    "Let's generate realistic telecom network performance data that represents typical enterprise telecom monitoring scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Save Python's built-in min/max functions before PySpark import overwrites them\n",
    "python_min = min\n",
    "python_max = max\n",
    "\n",
    "print(\"üì° Generating Synthetic Telecom Time Series Data...\")\n",
    "\n",
    "# Generation settings for demo (scaled down from 10GB)\n",
    "chunk_size_seconds = 60  # 1 minute chunks for demo\n",
    "site_count = 100  # number of unique cell sites (reduced for demo)\n",
    "start_time = datetime(2023, 1, 1)\n",
    "demo_chunks = 50  # Generate 50 chunks for demo (~5MB total)\n",
    "\n",
    "# Telecom network configuration\n",
    "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "cities = {\n",
    "    \"North\": [\"Dendam\", \"Rondburg\"],\n",
    "    \"South\": [\"Schieveste\"],\n",
    "    \"East\": [\"Schipstad\", \"Dort\"],\n",
    "    \"West\": [\"Damstad\"],\n",
    "}\n",
    "technologies = [\"6G\", \"7G\"]\n",
    "vendors = [\"Ericia\", \"Noson\", \"Weihu\"]\n",
    "\n",
    "# Generate site metadata\n",
    "print(\"üèóÔ∏è Creating telecom site metadata...\")\n",
    "sites = []\n",
    "for i in range(site_count):\n",
    "    region = random.choice(regions)\n",
    "    city = random.choice(cities[region])\n",
    "    tech = random.choices(technologies, weights=[0.5, 0.5])[0]\n",
    "    vendor = random.choices(vendors, weights=[0.4, 0.4, 0.2])[0]  # Weihu less common\n",
    "    site_id = f\"SITE_{i:05d}\"\n",
    "    sites.append({\n",
    "        \"site_id\": site_id,\n",
    "        \"region\": region,\n",
    "        \"city\": city,\n",
    "        \"technology\": tech,\n",
    "        \"vendor\": vendor,\n",
    "    })\n",
    "\n",
    "print(f\"üìç Created {len(sites)} telecom sites across {len(regions)} regions\")\n",
    "\n",
    "# Generate time series data for each site\n",
    "def generate_telecom_data_chunk(second_offset):\n",
    "    timestamp = start_time + timedelta(seconds=second_offset)\n",
    "    records = []\n",
    "    for site in sites:\n",
    "        region = site[\"region\"]\n",
    "        city = site[\"city\"]\n",
    "        tech = site[\"technology\"]\n",
    "        vendor = site[\"vendor\"]\n",
    "\n",
    "        # Signal strength (RSSI) - better for 7G and Noson vendor\n",
    "        base_rssi = -75 if tech == \"7G\" else -85\n",
    "        if vendor == \"Noson\":\n",
    "            base_rssi += 2\n",
    "        if vendor == \"Weihu\":\n",
    "            base_rssi -= 3\n",
    "        rssi = np.random.normal(loc=base_rssi, scale=3)\n",
    "\n",
    "        # Latency - varies by region and technology\n",
    "        if region == \"West\" or city in [\"Damstad\"]:\n",
    "            base_latency = 80\n",
    "        else:\n",
    "            base_latency = 35 if tech == \"7G\" else 60\n",
    "        latency = np.random.normal(loc=base_latency, scale=8)\n",
    "\n",
    "        # Data volume (in MB)\n",
    "        data_volume = np.random.exponential(scale=6)\n",
    "\n",
    "        # CPU usage - varies by technology and vendor\n",
    "        base_cpu = 48 + (7 if tech == \"7G\" else 0) + (5 if vendor == \"Weihu\" else 0)\n",
    "        cpu_usage = np.clip(np.random.normal(loc=base_cpu, scale=9), 0, 100)\n",
    "\n",
    "        # Drop rate - influenced by CPU and poor-performing cities\n",
    "        city_penalty = 0.04 if city in [\"Damstad\", \"Schieveste\"] else 0.01\n",
    "        drop_rate = (\n",
    "            python_min(1.0, 0.0012 * cpu_usage + city_penalty + np.random.beta(1, 180)) * 100\n",
    "        )\n",
    "\n",
    "        records.append({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"region\": region,\n",
    "            \"city\": city,\n",
    "            \"site_id\": site[\"site_id\"],\n",
    "            \"technology\": tech,\n",
    "            \"vendor\": vendor,\n",
    "            \"rssi_dbm\": round(rssi, 2),\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"data_volume_mb\": round(data_volume, 2),\n",
    "            \"drop_rate_percent\": round(drop_rate, 2),\n",
    "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
    "        })\n",
    "    return records\n",
    "\n",
    "# Generate telecom time series data\n",
    "print(\"‚ö° Generating telecom time series metrics...\")\n",
    "all_records = []\n",
    "for chunk_idx in range(demo_chunks):\n",
    "    chunk_records = []\n",
    "    for second in range(chunk_size_seconds):\n",
    "        minute_offset = chunk_idx * chunk_size_seconds + second\n",
    "        chunk_records.extend(generate_telecom_data_chunk(minute_offset))\n",
    "    all_records.extend(chunk_records)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "telecom_metrics_df = spark.createDataFrame(all_records)\n",
    "\n",
    "# Generate site metadata DataFrame\n",
    "sites_df = spark.createDataFrame(sites)\n",
    "\n",
    "print(\"‚úÖ Telecom data generated successfully!\")\n",
    "print(f\"   üì° Sites: {sites_df.count():,} unique telecom sites\")\n",
    "print(f\"   üìä Metrics: {telecom_metrics_df.count():,} time series records\")\n",
    "print(f\"   üïí Time Range: {start_time} to {start_time + timedelta(seconds=demo_chunks * chunk_size_seconds)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüìç Sample Site Metadata:\")\n",
    "sites_df.show(10)\n",
    "\n",
    "print(\"\\nüìä Sample Network Metrics:\")\n",
    "telecom_metrics_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Creating Iceberg Tables for Telecom Data\n",
    "\n",
    "Now let's create Iceberg tables optimized for telecom time-series data with enterprise-grade configurations including time-based partitioning for optimal query performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iceberg tables for telecom data\n",
    "print(\"üèóÔ∏è Creating Iceberg tables for telecom data...\")\n",
    "\n",
    "# Create telecom sites table (metadata/reference data)\n",
    "sites_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"local.db.telecom_sites\")\n",
    "\n",
    "# Create telecom metrics table with timestamp partitioning (critical for time-series data)\n",
    "telecom_metrics_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"timestamp\") \\\n",
    "    .saveAsTable(\"local.db.telecom_metrics\")\n",
    "\n",
    "print(\"‚úÖ Telecom Iceberg tables created successfully!\")\n",
    "\n",
    "# Verify tables\n",
    "print(\"\\nüìã Available Telecom Tables:\")\n",
    "spark.sql(\"SHOW TABLES IN local.db\").show()\n",
    "\n",
    "# Show table schemas\n",
    "print(\"\\nüèõÔ∏è Table Schemas:\")\n",
    "print(\"\\nüì° Telecom Sites Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.telecom_sites\").show()\n",
    "\n",
    "print(\"\\nüìä Telecom Metrics Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.telecom_metrics\").show()\n",
    "\n",
    "# Basic table statistics\n",
    "print(\"\\nüìä Telecom Table Statistics:\")\n",
    "sites_count = spark.sql(\"SELECT COUNT(*) as count FROM local.db.telecom_sites\").collect()[0]['count']\n",
    "metrics_count = spark.sql(\"SELECT COUNT(*) as count FROM local.db.telecom_metrics\").collect()[0]['count']\n",
    "\n",
    "# Calculate some basic telecom KPIs\n",
    "avg_rssi = spark.sql(\"SELECT AVG(rssi_dbm) as avg_rssi FROM local.db.telecom_metrics\").collect()[0]['avg_rssi']\n",
    "avg_latency = spark.sql(\"SELECT AVG(latency_ms) as avg_latency FROM local.db.telecom_metrics\").collect()[0]['avg_latency']\n",
    "avg_drop_rate = spark.sql(\"SELECT AVG(drop_rate_percent) as avg_drop_rate FROM local.db.telecom_metrics\").collect()[0]['avg_drop_rate']\n",
    "\n",
    "print(f\"   üì° Total Sites: {sites_count:,}\")\n",
    "print(f\"   üìä Metrics Records: {metrics_count:,}\")\n",
    "print(f\"   üì∂ Average RSSI: {avg_rssi:.2f} dBm\")\n",
    "print(f\"   ‚è±Ô∏è Average Latency: {avg_latency:.2f} ms\")\n",
    "print(f\"   ‚ö° Average Drop Rate: {avg_drop_rate:.2f}%\")\n",
    "\n",
    "# Show partition information for time-series table\n",
    "print(\"\\nüóÇÔ∏è Telecom Metrics Partitions:\")\n",
    "spark.sql(\"SELECT * FROM local.db.telecom_metrics.partitions LIMIT 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Time Travel - Telecom Network History Analysis\n",
    "\n",
    "Demonstrate Iceberg's powerful time travel capabilities for telecom network historical analysis, troubleshooting, and audit purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telecom Time Travel Demonstration\n",
    "print(\"üïê TELECOM NETWORK TIME TRAVEL DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# View table history\n",
    "print(\"\\nüìú Current Telecom Metrics Table History:\")\n",
    "spark.sql(\"SELECT * FROM local.db.telecom_metrics.history\").show(truncate=False)\n",
    "\n",
    "# Simulate network events - add more telecom data to create new snapshots\n",
    "print(\"\\nüîÑ Simulating network performance data updates...\")\n",
    "\n",
    "# Generate additional telecom metrics (simulating network changes)\n",
    "additional_records = []\n",
    "for chunk_idx in range(10):  # Add 10 more minutes of data\n",
    "    for second in range(60):\n",
    "        minute_offset = demo_chunks * chunk_size_seconds + chunk_idx * 60 + second\n",
    "        additional_records.extend(generate_telecom_data_chunk(minute_offset))\n",
    "\n",
    "additional_metrics_df = spark.createDataFrame(additional_records)\n",
    "\n",
    "# Append new telecom data\n",
    "additional_metrics_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"local.db.telecom_metrics\")\n",
    "\n",
    "print(f\"‚úÖ Added {additional_metrics_df.count()} new telecom metrics records\")\n",
    "\n",
    "# Simulate network configuration update\n",
    "print(\"\\nüì° Simulating network configuration update...\")\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE local.db.telecom_sites \n",
    "    SET technology = '7G' \n",
    "    WHERE technology = '6G' AND vendor = 'Noson' AND region = 'North'\n",
    "\"\"\")\n",
    "\n",
    "# Show updated history\n",
    "print(\"\\nüìú Updated Telecom Table History:\")\n",
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM local.db.telecom_metrics.history ORDER BY committed_at\").show(truncate=False)\n",
    "\n",
    "# Demonstrate time travel query for network analysis\n",
    "snapshots = spark.sql(\"SELECT snapshot_id FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\").collect()\n",
    "if len(snapshots) >= 2:\n",
    "    first_snapshot = snapshots[0]['snapshot_id']\n",
    "    print(f\"\\nüîç Analyzing network performance from historical snapshot: {first_snapshot}\")\n",
    "    \n",
    "    # Query historical network data\n",
    "    historical_data = spark.read \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .option(\"snapshot-id\", first_snapshot) \\\n",
    "        .table(\"local.db.telecom_metrics\")\n",
    "    \n",
    "    current_data = spark.read.format(\"iceberg\").table(\"local.db.telecom_metrics\")\n",
    "    \n",
    "    # Compare network performance metrics\n",
    "    historical_avg_latency = historical_data.select(avg(\"latency_ms\")).collect()[0][0]\n",
    "    current_avg_latency = current_data.select(avg(\"latency_ms\")).collect()[0][0]\n",
    "    \n",
    "    historical_avg_rssi = historical_data.select(avg(\"rssi_dbm\")).collect()[0][0]\n",
    "    current_avg_rssi = current_data.select(avg(\"rssi_dbm\")).collect()[0][0]\n",
    "    \n",
    "    print(f\"üìä Historical record count: {historical_data.count():,}\")\n",
    "    print(f\"üìä Current record count: {current_data.count():,}\")\n",
    "    print(f\"üìà New records added: {current_data.count() - historical_data.count():,}\")\n",
    "    print(f\"üì∂ Historical avg RSSI: {historical_avg_rssi:.2f} dBm\")\n",
    "    print(f\"üì∂ Current avg RSSI: {current_avg_rssi:.2f} dBm\")\n",
    "    print(f\"‚è±Ô∏è Historical avg latency: {historical_avg_latency:.2f} ms\")\n",
    "    print(f\"‚è±Ô∏è Current avg latency: {current_avg_latency:.2f} ms\")\n",
    "    \n",
    "    # Show network performance trend\n",
    "    print(\"\\nüìà Network Performance Comparison:\")\n",
    "    rssi_change = current_avg_rssi - historical_avg_rssi\n",
    "    latency_change = current_avg_latency - historical_avg_latency\n",
    "    print(f\"   RSSI Change: {rssi_change:+.2f} dBm ({'üìà Better' if rssi_change > 0 else 'üìâ Worse'})\")\n",
    "    print(f\"   Latency Change: {latency_change:+.2f} ms ({'üìà Better' if latency_change < 0 else 'üìâ Worse'})\")\n",
    "    \n",
    "print(\"\\n‚úÖ Telecom time travel demonstration completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Telecom Network Analytics & Best Practices\n",
    "\n",
    "Demonstrate real-world telecom network analytics scenarios and summarize enterprise best practices for telecom data lakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telecom Network Analytics Examples\n",
    "print(\"üì° TELECOM NETWORK ANALYTICS EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Network Performance by Region and Technology\n",
    "print(\"\\nüåç Network Performance by Region & Technology:\")\n",
    "regional_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.region,\n",
    "        s.technology,\n",
    "        COUNT(DISTINCT s.site_id) as site_count,\n",
    "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
    "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate,\n",
    "        ROUND(AVG(m.cpu_usage_percent), 2) as avg_cpu_usage\n",
    "    FROM local.db.telecom_sites s\n",
    "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
    "    GROUP BY s.region, s.technology\n",
    "    ORDER BY s.region, s.technology\n",
    "\"\"\")\n",
    "regional_analysis.show()\n",
    "\n",
    "# Vendor Performance Comparison\n",
    "print(\"\\nüè≠ Vendor Performance Comparison:\")\n",
    "vendor_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.vendor,\n",
    "        s.technology,\n",
    "        COUNT(DISTINCT s.site_id) as site_count,\n",
    "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
    "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate\n",
    "    FROM local.db.telecom_sites s\n",
    "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
    "    GROUP BY s.vendor, s.technology\n",
    "    ORDER BY avg_drop_rate ASC\n",
    "\"\"\")\n",
    "vendor_analysis.show()\n",
    "\n",
    "# Time-based Network Performance Trends\n",
    "print(\"\\nüìà Hourly Network Performance Trends:\")\n",
    "hourly_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        HOUR(m.timestamp) as hour_of_day,\n",
    "        COUNT(*) as measurement_count,\n",
    "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
    "        ROUND(AVG(m.data_volume_mb), 2) as avg_data_volume,\n",
    "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate\n",
    "    FROM local.db.telecom_metrics m\n",
    "    GROUP BY HOUR(m.timestamp)\n",
    "    ORDER BY hour_of_day\n",
    "\"\"\")\n",
    "hourly_trends.show()\n",
    "\n",
    "# Network Anomaly Detection (High Drop Rate Sites)\n",
    "print(\"\\n‚ö†Ô∏è Network Anomaly Detection (High Drop Rate Sites):\")\n",
    "anomaly_detection = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.site_id,\n",
    "        s.region,\n",
    "        s.city,\n",
    "        s.vendor,\n",
    "        s.technology,\n",
    "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate,\n",
    "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(m.latency_ms), 2) as avg_latency\n",
    "    FROM local.db.telecom_sites s\n",
    "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
    "    GROUP BY s.site_id, s.region, s.city, s.vendor, s.technology\n",
    "    HAVING AVG(m.drop_rate_percent) > 2.0\n",
    "    ORDER BY avg_drop_rate DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "anomaly_detection.show()\n",
    "\n",
    "# Telecom Enterprise Best Practices\n",
    "print(\"\\nüì° TELECOM ENTERPRISE BEST PRACTICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "telecom_practices = [\n",
    "    \"üïê Time-Series Partitioning:\",\n",
    "    \"   ‚Ä¢ Partition by timestamp (hourly/daily) for optimal query performance\",\n",
    "    \"   ‚Ä¢ Use hidden partitioning for automatic time-based partitioning\",\n",
    "    \"   ‚Ä¢ Consider site_id partitioning for geographically distributed queries\",\n",
    "    \"\",\n",
    "    \"üìä Network Monitoring:\",\n",
    "    \"   ‚Ä¢ Implement real-time streaming with batch processing\",\n",
    "    \"   ‚Ä¢ Set up automated anomaly detection on key KPIs\",\n",
    "    \"   ‚Ä¢ Use time travel for network incident analysis\",\n",
    "    \"\",\n",
    "    \"üîß Data Lifecycle Management:\",\n",
    "    \"   ‚Ä¢ Implement tiered storage (hot/warm/cold) based on data age\",\n",
    "    \"   ‚Ä¢ Regular compaction for time-series data efficiency\",\n",
    "    \"   ‚Ä¢ Automated cleanup of old snapshots based on retention policies\",\n",
    "    \"\",\n",
    "    \"üîí Regulatory Compliance:\",\n",
    "    \"   ‚Ä¢ Maintain audit trails for network performance data\",\n",
    "    \"   ‚Ä¢ Implement data lineage for regulatory reporting\",\n",
    "    \"   ‚Ä¢ Use schema evolution for changing network standards\",\n",
    "    \"\",\n",
    "    \"‚ö° Performance Optimization:\",\n",
    "    \"   ‚Ä¢ Use columnar format for analytical queries\",\n",
    "    \"   ‚Ä¢ Implement predicate pushdown for time-range queries\",\n",
    "    \"   ‚Ä¢ Optimize file sizes for network metric ingestion patterns\",\n",
    "    \"\",\n",
    "    \"üõ°Ô∏è Network Operations:\",\n",
    "    \"   ‚Ä¢ Backup critical network configuration metadata\",\n",
    "    \"   ‚Ä¢ Test disaster recovery for network monitoring systems\",\n",
    "    \"   ‚Ä¢ Use Iceberg ACID properties for consistent network reporting\"\n",
    "]\n",
    "\n",
    "for practice in telecom_practices:\n",
    "    print(practice)\n",
    "\n",
    "# Telecom Analytics Summary\n",
    "print(f\"\\n\\nüìä TELECOM DEMO SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "final_telecom_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Total Network Sites' as metric,\n",
    "        CAST(COUNT(*) AS STRING) as value\n",
    "    FROM local.db.telecom_sites\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Total Metric Records',\n",
    "        CAST(COUNT(*) AS STRING)\n",
    "    FROM local.db.telecom_metrics\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Average Network RSSI',\n",
    "        CONCAT(CAST(ROUND(AVG(rssi_dbm), 2) AS STRING), ' dBm')\n",
    "    FROM local.db.telecom_metrics\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Average Latency',\n",
    "        CONCAT(CAST(ROUND(AVG(latency_ms), 2) AS STRING), ' ms')\n",
    "    FROM local.db.telecom_metrics\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Average Drop Rate',\n",
    "        CONCAT(CAST(ROUND(AVG(drop_rate_percent), 2) AS STRING), '%')\n",
    "    FROM local.db.telecom_metrics\n",
    "\"\"\")\n",
    "final_telecom_stats.show(truncate=False)\n",
    "\n",
    "print(\"\\n‚úÖ Apache Iceberg Telecom Enterprise Demo Completed!\")\n",
    "print(\"üì° Ready for production telecom data lake deployment!\")\n",
    "print(\"üìö Learn more: https://iceberg.apache.org/\")\n",
    "\n",
    "# Cleanup\n",
    "print(\"\\nüßπ Cleaning up...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Apache Iceberg for On-Premise Enterprises\n",
    "## A Complete Guide with PySpark\n",
    "\n",
    "This notebook demonstrates how to use Apache Iceberg in on-premise enterprise environments using PySpark. Apache Iceberg is an open table format for huge analytic datasets that provides:\n",
    "\n",
    "- **ACID transactions**\n",
    "- **Schema evolution**\n",
    "- **Time travel**\n",
    "- **Hidden partitioning**\n",
    "- **Data compaction**\n",
    "- **Rollback capabilities**\n",
    "\n",
    "### Why Iceberg for Enterprises?\n",
    "- Reliable data lake operations\n",
    "- Multi-engine compatibility (Spark, Flink, Trino, etc.)\n",
    "- Better performance through advanced optimization\n",
    "- Enterprise-grade data governance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the required dependencies for Google Colab environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "Requirement already satisfied: pyspark==3.4.1 in /opt/anaconda3/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.12/site-packages (from pyspark==3.4.1) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: pyiceberg[s3fs,duckdb]==0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas==2.0.3\n",
      "  Using cached pandas-2.0.3.tar.gz (5.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.0.3) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.0.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.0.3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.0.3) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
      "Building wheels for collected packages: pandas\n",
      "  Building wheel for pandas (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandas: filename=pandas-2.0.3-cp312-cp312-macosx_11_0_arm64.whl size=10329889 sha256=ee7f217bea24162b3e3209723c5e401e2d0da8cdd062c8524bcb1804f75c557c\n",
      "  Stored in directory: /Users/codinggents/Library/Caches/pip/wheels/08/95/b7/15a2a9958c1fde0807c23b05bfed1a32ff9c7225c55d270d27\n",
      "Successfully built pandas\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install Java 8 (required for PySpark)\n",
    "!apt-get update\n",
    "!apt-get install -y openjdk-8-jdk-headless\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "# Install PySpark with Iceberg support\n",
    "%pip install pyspark==3.4.1\n",
    "%pip install pyiceberg[s3fs,duckdb]==0.5.1\n",
    "%pip install pandas==2.0.3\n",
    "%pip install matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Spark Configuration with Iceberg\n",
    "\n",
    "Configure Spark to work with Apache Iceberg. In enterprise environments, you would typically configure this in your cluster settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/iceberg-spark-runtime.jar: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-8-openjdk-amd64/bin/java: No such file or directory\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget -q https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar -O /content/iceberg-spark-runtime.jar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Configure Spark with Iceberg\u001b[39;00m\n\u001b[1;32m     11\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIceberg Enterprise Demo\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/iceberg-spark-runtime.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.extensions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.spark_catalog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.SparkSessionCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.spark_catalog.type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.local\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.SparkCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.local.type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhadoop\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.local.warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/iceberg-warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.warehouse.dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/iceberg-warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Set log level to reduce noise\u001b[39;00m\n\u001b[1;32m     24\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Download Iceberg JAR for Spark\n",
    "!wget -q https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar -O /content/iceberg-spark-runtime.jar\n",
    "\n",
    "# Configure Spark with Iceberg\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Enterprise Demo\") \\\n",
    "    .config(\"spark.jars\", \"/content/iceberg-spark-runtime.jar\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/content/iceberg-warehouse\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/content/iceberg-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark {spark.version} with Iceberg initialized successfully!\")\n",
    "print(f\"üìÅ Warehouse location: /content/iceberg-warehouse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Creating Sample Enterprise Data\n",
    "\n",
    "Let's create sample datasets that represent typical enterprise scenarios:\n",
    "- Customer data\n",
    "- Sales transactions\n",
    "- Product catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample customer data\n",
    "def generate_customer_data(num_customers=1000):\n",
    "    customers = []\n",
    "    for i in range(num_customers):\n",
    "        customers.append({\n",
    "            'customer_id': f'CUST_{i:06d}',\n",
    "            'first_name': f'FirstName{i}',\n",
    "            'last_name': f'LastName{i}',\n",
    "            'email': f'customer{i}@enterprise.com',\n",
    "            'registration_date': datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1400)),\n",
    "            'customer_segment': random.choice(['Premium', 'Standard', 'Basic']),\n",
    "            'credit_limit': random.randint(1000, 50000),\n",
    "            'country': random.choice(['USA', 'Canada', 'UK', 'Germany', 'France']),\n",
    "            'is_active': random.choice([True, False])\n",
    "        })\n",
    "    return customers\n",
    "\n",
    "# Generate sample sales data\n",
    "def generate_sales_data(num_transactions=5000):\n",
    "    sales = []\n",
    "    for i in range(num_transactions):\n",
    "        sales.append({\n",
    "            'transaction_id': f'TXN_{i:08d}',\n",
    "            'customer_id': f'CUST_{random.randint(0, 999):06d}',\n",
    "            'product_id': f'PROD_{random.randint(1, 100):03d}',\n",
    "            'transaction_date': datetime(2023, 1, 1) + timedelta(days=random.randint(0, 365)),\n",
    "            'quantity': random.randint(1, 10),\n",
    "            'unit_price': round(random.uniform(10.0, 500.0), 2),\n",
    "            'discount_percentage': random.uniform(0, 0.3),\n",
    "            'payment_method': random.choice(['Credit Card', 'Debit Card', 'Cash', 'Bank Transfer']),\n",
    "            'sales_rep': f'REP_{random.randint(1, 50):03d}'\n",
    "        })\n",
    "    return sales\n",
    "\n",
    "# Create DataFrames\n",
    "customers_data = generate_customer_data(1000)\n",
    "sales_data = generate_sales_data(5000)\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data)\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# Add calculated columns\n",
    "sales_df = sales_df.withColumn(\n",
    "    'total_amount', \n",
    "    col('quantity') * col('unit_price') * (1 - col('discount_percentage'))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample data generated:\")\n",
    "print(f\"   üìä Customers: {customers_df.count():,} records\")\n",
    "print(f\"   üí∞ Sales: {sales_df.count():,} transactions\")\n",
    "print(f\"   üíµ Total revenue: ${sales_df.select(sum('total_amount')).collect()[0][0]:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Creating Iceberg Tables\n",
    "\n",
    "Now let's create Iceberg tables. In enterprise environments, these would typically be stored in distributed storage systems like HDFS, S3, or Azure Data Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customers Iceberg table\n",
    "customers_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"/content/iceberg-warehouse/customers\") \\\n",
    "    .saveAsTable(\"local.db.customers\")\n",
    "\n",
    "# Create sales Iceberg table with partitioning (enterprise best practice)\n",
    "sales_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"/content/iceberg-warehouse/sales\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .saveAsTable(\"local.db.sales\")\n",
    "\n",
    "print(\"‚úÖ Iceberg tables created successfully!\")\n",
    "\n",
    "# Show table information\n",
    "print(\"\\nüìã Table Details:\")\n",
    "spark.sql(\"SHOW TABLES IN local.db\").show()\n",
    "\n",
    "# Show customers table schema\n",
    "print(\"\\nüë• Customers Table Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.customers\").show()\n",
    "\n",
    "# Show sales table schema\n",
    "print(\"\\nüí∞ Sales Table Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.sales\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Basic Iceberg Operations\n",
    "\n",
    "Let's explore basic operations that are crucial for enterprise data management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Iceberg tables\n",
    "customers_iceberg = spark.read.format(\"iceberg\").table(\"local.db.customers\")\n",
    "sales_iceberg = spark.read.format(\"iceberg\").table(\"local.db.sales\")\n",
    "\n",
    "print(\"üìä Data Summary:\")\n",
    "print(f\"   Customers: {customers_iceberg.count():,}\")\n",
    "print(f\"   Sales Transactions: {sales_iceberg.count():,}\")\n",
    "\n",
    "# Sample queries\n",
    "print(\"\\nüîç Top 5 Customer Segments by Count:\")\n",
    "customers_iceberg.groupBy(\"customer_segment\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\nüí≥ Sales by Payment Method:\")\n",
    "sales_iceberg.groupBy(\"payment_method\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        round(sum(\"total_amount\"), 2).alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc()) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Time Travel - Enterprise Data Recovery\n",
    "\n",
    "One of Iceberg's most powerful features for enterprises is time travel, allowing you to query data as it existed at any point in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table history\n",
    "print(\"üìú Sales Table History:\")\n",
    "spark.sql(\"SELECT * FROM local.db.sales.history\").show(truncate=False)\n",
    "\n",
    "# Get current snapshot info\n",
    "print(\"\\nüì∏ Current Snapshots:\")\n",
    "spark.sql(\"SELECT * FROM local.db.sales.snapshots\").show(truncate=False)\n",
    "\n",
    "# Make some changes to demonstrate time travel\n",
    "print(\"\\nüîÑ Making changes to demonstrate time travel...\")\n",
    "\n",
    "# Add new sales data (simulating new transactions)\n",
    "new_sales_data = generate_sales_data(100)\n",
    "new_sales_df = spark.createDataFrame(new_sales_data)\n",
    "new_sales_df = new_sales_df.withColumn(\n",
    "    'total_amount', \n",
    "    col('quantity') * col('unit_price') * (1 - col('discount_percentage'))\n",
    ")\n",
    "\n",
    "# Append to existing table\n",
    "new_sales_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"local.db.sales\")\n",
    "\n",
    "print(f\"‚úÖ Added {new_sales_df.count()} new transactions\")\n",
    "print(f\"üìä Total transactions now: {spark.read.format('iceberg').table('local.db.sales').count():,}\")\n",
    "\n",
    "# Show updated history\n",
    "print(\"\\nüìú Updated Table History:\")\n",
    "spark.sql(\"SELECT * FROM local.db.sales.history ORDER BY made_current_at\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Schema Evolution - Enterprise Agility\n",
    "\n",
    "Schema evolution allows enterprises to adapt their data structures without breaking existing pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current schema\n",
    "print(\"üìã Current Customers Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.customers\").show()\n",
    "\n",
    "# Add a new column (common enterprise requirement)\n",
    "print(\"\\nüîß Adding new column: loyalty_points\")\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.db.customers \n",
    "    ADD COLUMN loyalty_points INT AFTER credit_limit\n",
    "\"\"\")\n",
    "\n",
    "# Show updated schema\n",
    "print(\"\\nüìã Updated Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.customers\").show()\n",
    "\n",
    "# Update some records with loyalty points\n",
    "print(\"\\nüìù Updating loyalty points for Premium customers...\")\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE local.db.customers \n",
    "    SET loyalty_points = CAST(credit_limit * 0.1 AS INT)\n",
    "    WHERE customer_segment = 'Premium'\n",
    "\"\"\")\n",
    "\n",
    "# Verify the update\n",
    "print(\"\\n‚úÖ Premium customers with loyalty points:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT customer_segment, \n",
    "           COUNT(*) as customer_count,\n",
    "           AVG(loyalty_points) as avg_loyalty_points\n",
    "    FROM local.db.customers \n",
    "    WHERE customer_segment = 'Premium'\n",
    "    GROUP BY customer_segment\n",
    "\"\"\").show()\n",
    "\n",
    "# Show that old queries still work (backward compatibility)\n",
    "print(\"\\nüîÑ Backward compatibility check - old queries still work:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT customer_segment, COUNT(*) as count\n",
    "    FROM local.db.customers \n",
    "    GROUP BY customer_segment\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Enterprise Best Practices Summary\n",
    "\n",
    "Key recommendations for using Apache Iceberg in enterprise environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best practices and final summary\n",
    "print(\"üè¢ ENTERPRISE APACHE ICEBERG BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_practices = [\n",
    "    \"üéØ **Partitioning Strategy**\",\n",
    "    \"   ‚Ä¢ Use date/time partitioning for time-series data\",\n",
    "    \"   ‚Ä¢ Consider business-specific partitions (region, department)\",\n",
    "    \"   ‚Ä¢ Avoid over-partitioning (aim for 100MB+ per partition)\",\n",
    "    \"\",\n",
    "    \"üîß **Table Maintenance**\",\n",
    "    \"   ‚Ä¢ Schedule regular compaction jobs\",\n",
    "    \"   ‚Ä¢ Implement snapshot cleanup policies\",\n",
    "    \"   ‚Ä¢ Monitor table statistics and file counts\",\n",
    "    \"\",\n",
    "    \"üîí **Data Governance**\",\n",
    "    \"   ‚Ä¢ Use schema evolution carefully with proper testing\",\n",
    "    \"   ‚Ä¢ Implement data lineage tracking\",\n",
    "    \"   ‚Ä¢ Set up proper access controls and auditing\",\n",
    "    \"\",\n",
    "    \"üìä **Performance Optimization**\",\n",
    "    \"   ‚Ä¢ Use vectorized readers when available\",\n",
    "    \"   ‚Ä¢ Implement predicate pushdown in queries\",\n",
    "    \"   ‚Ä¢ Optimize file sizes (128MB-1GB per file)\",\n",
    "    \"\",\n",
    "    \"üõ°Ô∏è **Reliability & Recovery**\",\n",
    "    \"   ‚Ä¢ Implement backup strategies for metadata\",\n",
    "    \"   ‚Ä¢ Test disaster recovery procedures\",\n",
    "    \"   ‚Ä¢ Use time travel for audit and compliance\",\n",
    "    \"\",\n",
    "    \"üîó **Integration**\",\n",
    "    \"   ‚Ä¢ Standardize on Iceberg across analytics engines\",\n",
    "    \"   ‚Ä¢ Implement proper CI/CD for schema changes\",\n",
    "    \"   ‚Ä¢ Use catalog services for metadata management\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(practice)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\\nüìà DEMO SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "summary_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Total Customers' as metric,\n",
    "        CAST(COUNT(*) AS STRING) as value\n",
    "    FROM local.db.customers\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Total Sales Transactions',\n",
    "        CAST(COUNT(*) AS STRING)\n",
    "    FROM local.db.sales\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Total Revenue',\n",
    "        CONCAT('$', CAST(ROUND(SUM(total_amount), 2) AS STRING))\n",
    "    FROM local.db.sales\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Active Customers',\n",
    "        CAST(SUM(CASE WHEN is_active THEN 1 ELSE 0 END) AS STRING)\n",
    "    FROM local.db.customers\n",
    "\"\"\")\n",
    "\n",
    "summary_stats.show(truncate=False)\n",
    "\n",
    "print(\"\\n‚úÖ Apache Iceberg Enterprise Demo Completed Successfully!\")\n",
    "print(\"\\nüöÄ Ready for production deployment with proper configuration!\")\n",
    "\n",
    "# Optional cleanup\n",
    "print(\"\\n‚èπÔ∏è Stopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Demo completed! Your Iceberg tables are preserved in /content/iceberg-warehouse/\")\n",
    "print(\"üìö To learn more, visit: https://iceberg.apache.org/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
