{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Apache Iceberg for Telecom Enterprises (Linux)\n",
        "## A Complete Guide with PySpark & Telecom Time Series Data\n",
        "\n",
        "This notebook demonstrates how to use Apache Iceberg in telecom enterprise environments using PySpark on **Linux**.\n",
        "\n",
        "### Key Features:\n",
        "- **ACID transactions** for reliable telecom data operations\n",
        "- **Schema evolution** for adapting to new network technologies\n",
        "- **Time travel** for historical network performance analysis\n",
        "- **Hidden partitioning** for optimal time-series data queries\n",
        "- **Data compaction** for efficient storage of large telecom datasets\n",
        "\n",
        "### Linux Requirements:\n",
        "- Java 8+ (install via `sudo apt-get install openjdk-8-jdk`)\n",
        "- Python 3.8+\n",
        "- Sufficient disk space for demo data\n",
        "\n",
        "### Supported Linux Distributions:\n",
        "- Ubuntu 18.04+\n",
        "- CentOS 7+\n",
        "- Red Hat Enterprise Linux 7+\n",
        "- Debian 9+\n",
        "\n",
        "**Note:** This notebook is optimized specifically for Linux environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "print(\"üêß Linux Apache Iceberg Setup\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Detect Linux distribution\n",
        "try:\n",
        "    with open('/etc/os-release', 'r') as f:\n",
        "        os_info = f.read()\n",
        "        if 'ubuntu' in os_info.lower():\n",
        "            distro = 'ubuntu'\n",
        "        elif 'centos' in os_info.lower():\n",
        "            distro = 'centos'\n",
        "        elif 'debian' in os_info.lower():\n",
        "            distro = 'debian'\n",
        "        elif 'rhel' in os_info.lower():\n",
        "            distro = 'rhel'\n",
        "        else:\n",
        "            distro = 'unknown'\n",
        "    print(f\"üñ•Ô∏è Linux Distribution: {distro}\")\n",
        "except:\n",
        "    distro = 'unknown'\n",
        "    print(\"üñ•Ô∏è Linux Distribution: unknown\")\n",
        "\n",
        "# Check for Java installation on Linux\n",
        "try:\n",
        "    java_version = subprocess.run(['java', '-version'], capture_output=True, text=True, stderr=subprocess.STDOUT)\n",
        "    if java_version.returncode == 0:\n",
        "        print(\"‚òï Java is installed\")\n",
        "        \n",
        "        # Try to find JAVA_HOME on Linux\n",
        "        java_home_candidates = [\n",
        "            '/usr/lib/jvm/java-8-openjdk-amd64',\n",
        "            '/usr/lib/jvm/java-11-openjdk-amd64',\n",
        "            '/usr/lib/jvm/default-java',\n",
        "            '/usr/java/default',\n",
        "            '/opt/java'\n",
        "        ]\n",
        "        \n",
        "        java_home_found = False\n",
        "        for candidate in java_home_candidates:\n",
        "            if os.path.exists(candidate):\n",
        "                os.environ[\"JAVA_HOME\"] = candidate\n",
        "                print(f\"üè† JAVA_HOME: {candidate}\")\n",
        "                java_home_found = True\n",
        "                break\n",
        "        \n",
        "        if not java_home_found:\n",
        "            print(\"‚ö†Ô∏è Could not determine JAVA_HOME automatically\")\n",
        "            print(\"üí° Please set JAVA_HOME manually in your environment\")\n",
        "            \n",
        "    else:\n",
        "        print(\"‚ùå Java not found!\")\n",
        "        if distro in ['ubuntu', 'debian']:\n",
        "            print(\"üí° Install Java using: sudo apt-get update && sudo apt-get install openjdk-8-jdk\")\n",
        "        elif distro in ['centos', 'rhel']:\n",
        "            print(\"üí° Install Java using: sudo yum install java-1.8.0-openjdk-devel\")\n",
        "        else:\n",
        "            print(\"üí° Install Java 8+ for your Linux distribution\")\n",
        "        sys.exit(1)\n",
        "        \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Java not found!\")\n",
        "    if distro in ['ubuntu', 'debian']:\n",
        "        print(\"üí° Install Java using: sudo apt-get update && sudo apt-get install openjdk-8-jdk\")\n",
        "    elif distro in ['centos', 'rhel']:\n",
        "        print(\"üí° Install Java using: sudo yum install java-1.8.0-openjdk-devel\")\n",
        "    else:\n",
        "        print(\"üí° Install Java 8+ for your Linux distribution\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"‚úÖ Java setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python packages for Linux\n",
        "print(\"üì¶ Installing Python packages for Linux...\")\n",
        "\n",
        "%pip install -q pyspark==3.4.1\n",
        "%pip install -q pyiceberg[s3fs]==0.5.1\n",
        "%pip install -q pandas>=2.0.0\n",
        "%pip install -q numpy>=1.21.0\n",
        "%pip install -q matplotlib seaborn\n",
        "\n",
        "print(\"‚úÖ Package installation completed!\")\n",
        "\n",
        "# Test imports\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pyspark\n",
        "    print(f\"üìä Pandas: {pd.__version__}\")\n",
        "    print(f\"üî¢ NumPy: {np.__version__}\")\n",
        "    print(f\"‚ö° PySpark: {pyspark.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    \n",
        "print(\"üöÄ Linux setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spark Configuration for Linux\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, sum, avg, count\n",
        "import os\n",
        "\n",
        "# Download Iceberg JAR for Linux\n",
        "jar_url = \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar\"\n",
        "jar_path = \"./iceberg-spark-runtime.jar\"\n",
        "warehouse_path = \"./iceberg-warehouse\"\n",
        "\n",
        "# S3 configuration (commented out - uncomment and configure for S3)\n",
        "# s3_warehouse_path = \"s3a://your-iceberg-bucket/your-warehouse-path\"\n",
        "# s3_access_key = \"YOUR_AWS_ACCESS_KEY_ID\"\n",
        "# s3_secret_key = \"YOUR_AWS_SECRET_ACCESS_KEY\"\n",
        "# s3_hadoop_jar = \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar\"\n",
        "# s3_bundle_jar = \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar\"\n",
        "\n",
        "print(\"üì• Downloading Iceberg JAR for Linux...\")\n",
        "!wget -q {jar_url} -O {jar_path}\n",
        "\n",
        "# Download S3 related JARs if using S3 (uncomment below if using S3)\n",
        "# if 's3_hadoop_jar' in locals() and not os.path.exists(\"./hadoop-aws.jar\"):\n",
        "#     print(\"üì• Downloading Hadoop AWS JAR for Linux...\")\n",
        "#     !wget -q {s3_hadoop_jar} -O ./hadoop-aws.jar\n",
        "# if 's3_bundle_jar' in locals() and not os.path.exists(\"./aws-java-sdk-bundle.jar\"):\n",
        "#     print(\"üì• Downloading AWS SDK Bundle JAR for Linux...\")\n",
        "#     !wget -q {s3_bundle_jar} -O ./aws-java-sdk-bundle.jar\n",
        "\n",
        "# Configure Spark with Iceberg for Linux\n",
        "print(\"‚ö° Initializing Spark with Iceberg on Linux...\")\n",
        "\n",
        "spark_builder = SparkSession.builder \\\n",
        "    .appName(\"Iceberg Telecom Demo - Linux\") \\\n",
        "    .config(\"spark.jars\", jar_path) \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\")\n",
        "\n",
        "# Configure for local warehouse\n",
        "spark_builder = spark_builder.config(\"spark.sql.catalog.local.warehouse\", warehouse_path) \\\n",
        "                             .config(\"spark.sql.warehouse.dir\", warehouse_path)\n",
        "\n",
        "# Configure for S3 warehouse (commented out - uncomment and configure for S3)\n",
        "# spark_builder = spark_builder.config(\"spark.sql.catalog.local.warehouse\", s3_warehouse_path) \\\n",
        "#                              .config(\"spark.sql.warehouse.dir\", s3_warehouse_path) \\\n",
        "#                              .config(\"spark.jars\", f\"{jar_path},./hadoop-aws.jar,./aws-java-sdk-bundle.jar\") \\\n",
        "#                              .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
        "#                              .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
        "#                              .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "\n",
        "spark = spark_builder.getOrCreate()\n",
        "\n",
        "# Set log level to reduce noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"‚úÖ Spark {spark.version} with Iceberg initialized on Linux!\")\n",
        "print(f\"üìÅ Warehouse: {warehouse_path}\")\n",
        "print(f\"‚òï Java Home: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
        "\n",
        "# Generate Telecom Data for Linux\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "print(\"\\nüì° Generating Telecom Data for Linux...\")\n",
        "\n",
        "# Linux configuration (full-scale demo)\n",
        "chunk_size_seconds = 60\n",
        "site_count = 200  # Larger dataset for Linux\n",
        "start_time = datetime(2023, 1, 1)\n",
        "demo_chunks = 100  # More data for Linux\n",
        "\n",
        "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
        "cities = {\n",
        "    \"North\": [\"Dendam\", \"Rondburg\", \"Nordville\"],\n",
        "    \"South\": [\"Schieveste\", \"Southpark\"],\n",
        "    \"East\": [\"Schipstad\", \"Dort\", \"Eastport\"],\n",
        "    \"West\": [\"Damstad\", \"Westfield\"],\n",
        "    \"Central\": [\"Centrum\", \"Midtown\"]\n",
        "}\n",
        "technologies = [\"6G\", \"7G\", \"8G\"]  # More technologies for Linux\n",
        "vendors = [\"Ericia\", \"Noson\", \"Weihu\", \"Samsong\"]  # More vendors\n",
        "\n",
        "# Generate site metadata\n",
        "sites = []\n",
        "for i in range(site_count):\n",
        "    region = random.choice(regions)\n",
        "    city = random.choice(cities[region])\n",
        "    tech = random.choices(technologies, weights=[0.3, 0.5, 0.2])[0]\n",
        "    vendor = random.choices(vendors, weights=[0.3, 0.3, 0.2, 0.2])[0]\n",
        "    site_id = f\"SITE_{i:05d}\"\n",
        "    sites.append({\n",
        "        \"site_id\": site_id,\n",
        "        \"region\": region,\n",
        "        \"city\": city,\n",
        "        \"technology\": tech,\n",
        "        \"vendor\": vendor,\n",
        "    })\n",
        "\n",
        "def generate_telecom_data_chunk(second_offset):\n",
        "    timestamp = start_time + timedelta(seconds=second_offset)\n",
        "    records = []\n",
        "    for site in sites:\n",
        "        region = site[\"region\"]\n",
        "        city = site[\"city\"]\n",
        "        tech = site[\"technology\"]\n",
        "        vendor = site[\"vendor\"]\n",
        "\n",
        "        # Enhanced signal strength (RSSI) for Linux\n",
        "        base_rssi = -70 if tech == \"8G\" else (-75 if tech == \"7G\" else -85)\n",
        "        if vendor == \"Noson\":\n",
        "            base_rssi += 3\n",
        "        elif vendor == \"Weihu\":\n",
        "            base_rssi -= 2\n",
        "        elif vendor == \"Samsong\":\n",
        "            base_rssi += 1\n",
        "        rssi = np.random.normal(loc=base_rssi, scale=3)\n",
        "\n",
        "        # Enhanced latency modeling\n",
        "        if region == \"Central\":\n",
        "            base_latency = 25 if tech == \"8G\" else (35 if tech == \"7G\" else 60)\n",
        "        else:\n",
        "            base_latency = 30 if tech == \"8G\" else (40 if tech == \"7G\" else 65)\n",
        "        latency = np.random.normal(loc=base_latency, scale=8)\n",
        "\n",
        "        # Data volume (in MB)\n",
        "        data_volume = np.random.exponential(scale=8)\n",
        "\n",
        "        # CPU usage with vendor-specific characteristics\n",
        "        base_cpu = 45 + (5 if tech == \"8G\" else (7 if tech == \"7G\" else 0))\n",
        "        if vendor == \"Weihu\":\n",
        "            base_cpu += 8\n",
        "        elif vendor == \"Samsong\":\n",
        "            base_cpu += 3\n",
        "        cpu_usage = np.clip(np.random.normal(loc=base_cpu, scale=10), 0, 100)\n",
        "\n",
        "        # Drop rate calculation\n",
        "        city_penalty = 0.03 if city in [\"Damstad\", \"Schieveste\"] else 0.008\n",
        "        vendor_penalty = 0.015 if vendor == \"Weihu\" else 0.005\n",
        "        drop_rate = (\n",
        "            min(1.0, 0.001 * cpu_usage + city_penalty + vendor_penalty + np.random.beta(1, 200)) * 100\n",
        "        )\n",
        "\n",
        "        records.append({\n",
        "            \"timestamp\": timestamp,\n",
        "            \"region\": region,\n",
        "            \"city\": city,\n",
        "            \"site_id\": site[\"site_id\"],\n",
        "            \"technology\": tech,\n",
        "            \"vendor\": vendor,\n",
        "            \"rssi_dbm\": round(rssi, 2),\n",
        "            \"latency_ms\": round(latency, 2),\n",
        "            \"data_volume_mb\": round(data_volume, 2),\n",
        "            \"drop_rate_percent\": round(drop_rate, 2),\n",
        "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
        "        })\n",
        "    return records\n",
        "\n",
        "# Generate telecom time series data\n",
        "print(\"‚ö° Generating comprehensive telecom metrics for Linux...\")\n",
        "all_records = []\n",
        "for chunk_idx in range(demo_chunks):\n",
        "    chunk_records = []\n",
        "    for second in range(chunk_size_seconds):\n",
        "        minute_offset = chunk_idx * chunk_size_seconds + second\n",
        "        chunk_records.extend(generate_telecom_data_chunk(minute_offset))\n",
        "    all_records.extend(chunk_records)\n",
        "\n",
        "# Convert to Spark DataFrames\n",
        "telecom_metrics_df = spark.createDataFrame(all_records)\n",
        "sites_df = spark.createDataFrame(sites)\n",
        "\n",
        "print(\"‚úÖ Linux telecom data generated!\")\n",
        "print(f\"   üì° Sites: {sites_df.count():,}\")\n",
        "print(f\"   üìä Metrics: {telecom_metrics_df.count():,}\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nüìä Sample Telecom Data:\")\n",
        "telecom_metrics_df.show(10)\n",
        "sites_df.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Iceberg Tables for Linux Production Environment\n",
        "print(\"üèóÔ∏è Creating Iceberg tables for Linux...\")\n",
        "\n",
        "# Create telecom sites table\n",
        "sites_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"local.db.telecom_sites\")\n",
        "\n",
        "# Create telecom metrics table with timestamp partitioning\n",
        "telecom_metrics_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"timestamp\") \\\n",
        "    .saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"‚úÖ Iceberg tables created for Linux production!\")\n",
        "\n",
        "# Verify tables\n",
        "spark.sql(\"SHOW TABLES IN local.db\").show()\n",
        "\n",
        "# Comprehensive Linux Analytics\n",
        "print(\"\\nüìä Comprehensive Telecom Analytics:\")\n",
        "\n",
        "# Performance by vendor and technology\n",
        "print(\"\\nüè¢ Vendor Performance Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.vendor,\n",
        "        s.technology,\n",
        "        COUNT(DISTINCT s.site_id) as sites,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
        "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate,\n",
        "        ROUND(AVG(m.cpu_usage_percent), 2) as avg_cpu\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.vendor, s.technology\n",
        "    ORDER BY s.vendor, s.technology\n",
        "\"\"\").show()\n",
        "\n",
        "# Regional performance analysis\n",
        "print(\"\\nüåç Regional Performance Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.region,\n",
        "        COUNT(DISTINCT s.site_id) as sites,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
        "        ROUND(SUM(m.data_volume_mb), 2) as total_data_gb\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.region\n",
        "    ORDER BY total_data_gb DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# Time travel and schema evolution demo\n",
        "print(\"\\nüïê Advanced Iceberg Features for Linux:\")\n",
        "\n",
        "# Show all snapshots\n",
        "snapshots = spark.sql(\"SELECT snapshot_id, committed_at FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\").collect()\n",
        "print(f\"üì∏ Available snapshots: {len(snapshots)}\")\n",
        "\n",
        "# Schema evolution example\n",
        "print(\"\\nüîÑ Schema Evolution Example:\")\n",
        "spark.sql(\"ALTER TABLE local.db.telecom_metrics ADD COLUMN network_load_percent DOUBLE\").collect()\n",
        "print(\"   ‚úÖ Added network_load_percent column\")\n",
        "\n",
        "# Add sample data with new column\n",
        "from pyspark.sql.functions import lit\n",
        "updated_df = telecom_metrics_df.withColumn(\"network_load_percent\", lit(np.random.uniform(20, 80)))\n",
        "updated_df.limit(1000).write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"   ‚úÖ Appended data with new schema\")\n",
        "\n",
        "# Show updated schema\n",
        "print(\"\\nüìã Updated Table Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.telecom_metrics\").show()\n",
        "\n",
        "# Performance optimization\n",
        "print(\"\\n‚ö° Performance Optimization:\")\n",
        "spark.sql(\"CALL local.system.rewrite_data_files('local.db.telecom_metrics')\").collect()\n",
        "print(\"   ‚úÖ Data files compacted\")\n",
        "\n",
        "print(\"\\n‚úÖ Linux Enterprise Iceberg Demo Completed!\")\n",
        "print(\"üöÄ Ready for production telecom data lake deployment!\")\n",
        "\n",
        "# Show final statistics\n",
        "total_records = spark.sql(\"SELECT COUNT(*) as total FROM local.db.telecom_metrics\").collect()[0]['total']\n",
        "print(f\"üìä Total telecom records: {total_records:,}\")\n",
        "\n",
        "# Optional: Show table size\n",
        "table_files = spark.sql(\"SELECT COUNT(*) as files FROM local.db.telecom_metrics.files\").collect()[0]['files']\n",
        "print(f\"üìÅ Total data files: {table_files}\")\n",
        "\n",
        "# Cleanup option (uncomment if needed)\n",
        "# spark.stop()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Basic Iceberg Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Iceberg Operations - Linux Enterprise\n",
        "print(\"üîß Basic Iceberg Operations (Linux)\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# 1. Comprehensive Table Information\n",
        "print(\"\\nüìã Detailed Table Information:\")\n",
        "spark.sql(\"DESCRIBE EXTENDED local.db.telecom_metrics\").show(truncate=False)\n",
        "\n",
        "# 2. Table Properties and Configuration\n",
        "print(\"\\n‚öôÔ∏è Table Properties:\")\n",
        "spark.sql(\"SHOW TBLPROPERTIES local.db.telecom_metrics\").show(truncate=False)\n",
        "\n",
        "# 3. Partition Information\n",
        "print(\"\\nüìä Partition Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        DATE(timestamp) as partition_date,\n",
        "        COUNT(*) as records,\n",
        "        COUNT(DISTINCT site_id) as unique_sites,\n",
        "        ROUND(SUM(data_volume_mb)/1024, 2) as total_data_gb,\n",
        "        ROUND(AVG(rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(latency_ms), 2) as avg_latency\n",
        "    FROM local.db.telecom_metrics \n",
        "    GROUP BY DATE(timestamp)\n",
        "    ORDER BY partition_date\n",
        "    LIMIT 15\n",
        "\"\"\").show()\n",
        "\n",
        "# 4. Advanced Filtering and Window Functions\n",
        "print(\"\\nüîç Advanced Query Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.site_id,\n",
        "        s.region,\n",
        "        s.vendor,\n",
        "        s.technology,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
        "        ROUND(AVG(m.cpu_usage_percent), 2) as avg_cpu,\n",
        "        ROW_NUMBER() OVER (PARTITION BY s.region ORDER BY AVG(m.rssi_dbm) DESC) as region_rank\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.site_id, s.region, s.vendor, s.technology\n",
        "    HAVING AVG(m.rssi_dbm) > -80\n",
        "    ORDER BY s.region, region_rank\n",
        "    LIMIT 20\n",
        "\"\"\").show()\n",
        "\n",
        "# 5. Data Quality Checks\n",
        "print(\"\\n‚úÖ Data Quality Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        'Total Records' as metric,\n",
        "        COUNT(*) as value\n",
        "    FROM local.db.telecom_metrics\n",
        "    \n",
        "    UNION ALL\n",
        "    \n",
        "    SELECT \n",
        "        'Records with NULL RSSI' as metric,\n",
        "        COUNT(*) as value\n",
        "    FROM local.db.telecom_metrics\n",
        "    WHERE rssi_dbm IS NULL\n",
        "    \n",
        "    UNION ALL\n",
        "    \n",
        "    SELECT \n",
        "        'Records with Extreme Latency (>200ms)' as metric,\n",
        "        COUNT(*) as value\n",
        "    FROM local.db.telecom_metrics\n",
        "    WHERE latency_ms > 200\n",
        "    \n",
        "    UNION ALL\n",
        "    \n",
        "    SELECT \n",
        "        'Records with High Drop Rate (>10%)' as metric,\n",
        "        COUNT(*) as value\n",
        "    FROM local.db.telecom_metrics\n",
        "    WHERE drop_rate_percent > 10\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"‚úÖ Basic operations analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Time Travel & Snapshots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Time Travel & Snapshots - Linux Enterprise\n",
        "print(\"üïê Time Travel & Snapshots (Linux Enterprise)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Comprehensive Snapshot Analysis\n",
        "print(\"\\nüì∏ Detailed Snapshot Information:\")\n",
        "snapshots_df = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        snapshot_id,\n",
        "        committed_at,\n",
        "        summary,\n",
        "        manifest_list,\n",
        "        schema_id\n",
        "    FROM local.db.telecom_metrics.snapshots \n",
        "    ORDER BY committed_at\n",
        "\"\"\")\n",
        "snapshots_df.show(truncate=False)\n",
        "\n",
        "snapshots = snapshots_df.collect()\n",
        "print(f\"Total snapshots available: {len(snapshots)}\")\n",
        "\n",
        "# 2. Create multiple snapshots with different data patterns\n",
        "print(\"\\n‚ûï Creating Multiple Snapshots for Time Travel Demo...\")\n",
        "\n",
        "# Snapshot 1: Network upgrade simulation\n",
        "print(\"   üì° Simulating network upgrade...\")\n",
        "upgrade_records = []\n",
        "current_time = datetime.now()\n",
        "\n",
        "for site in sites[:50]:  # Simulate upgrade for 50 sites\n",
        "    # Improved metrics after upgrade\n",
        "    base_rssi = -65 if site[\"technology\"] == \"8G\" else -70\n",
        "    base_latency = 25 if site[\"technology\"] == \"8G\" else 30\n",
        "    \n",
        "    upgrade_records.append({\n",
        "        \"timestamp\": current_time,\n",
        "        \"region\": site[\"region\"],\n",
        "        \"city\": site[\"city\"],\n",
        "        \"site_id\": site[\"site_id\"],\n",
        "        \"technology\": site[\"technology\"],\n",
        "        \"vendor\": site[\"vendor\"],\n",
        "        \"rssi_dbm\": round(np.random.normal(loc=base_rssi, scale=2), 2),\n",
        "        \"latency_ms\": round(np.random.normal(loc=base_latency, scale=5), 2),\n",
        "        \"data_volume_mb\": round(np.random.exponential(scale=10), 2),\n",
        "        \"drop_rate_percent\": round(np.random.beta(1, 300) * 100, 2),\n",
        "        \"cpu_usage_percent\": round(np.clip(np.random.normal(loc=40, scale=8), 0, 100), 2),\n",
        "    })\n",
        "\n",
        "upgrade_df = spark.createDataFrame(upgrade_records)\n",
        "upgrade_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.telecom_metrics\")\n",
        "print(\"   ‚úÖ Network upgrade data added\")\n",
        "\n",
        "# Snapshot 2: Peak traffic simulation\n",
        "print(\"   üìà Simulating peak traffic period...\")\n",
        "peak_records = []\n",
        "peak_time = current_time + timedelta(hours=1)\n",
        "\n",
        "for site in sites[:30]:  # Peak traffic affects 30 sites\n",
        "    # Degraded performance during peak\n",
        "    degraded_rssi = np.random.normal(loc=-80, scale=4)\n",
        "    degraded_latency = np.random.normal(loc=70, scale=15)\n",
        "    \n",
        "    peak_records.append({\n",
        "        \"timestamp\": peak_time,\n",
        "        \"region\": site[\"region\"],\n",
        "        \"city\": site[\"city\"],\n",
        "        \"site_id\": site[\"site_id\"],\n",
        "        \"technology\": site[\"technology\"],\n",
        "        \"vendor\": site[\"vendor\"],\n",
        "        \"rssi_dbm\": round(degraded_rssi, 2),\n",
        "        \"latency_ms\": round(degraded_latency, 2),\n",
        "        \"data_volume_mb\": round(np.random.exponential(scale=15), 2),\n",
        "        \"drop_rate_percent\": round(np.random.beta(1, 100) * 100, 2),\n",
        "        \"cpu_usage_percent\": round(np.clip(np.random.normal(loc=80, scale=10), 0, 100), 2),\n",
        "    })\n",
        "\n",
        "peak_df = spark.createDataFrame(peak_records)\n",
        "peak_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.telecom_metrics\")\n",
        "print(\"   ‚úÖ Peak traffic data added\")\n",
        "\n",
        "# 3. Show updated snapshots\n",
        "print(\"\\nüì∏ Updated Snapshot Timeline:\")\n",
        "updated_snapshots = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        snapshot_id,\n",
        "        committed_at,\n",
        "        summary['added-records'] as added_records,\n",
        "        summary['total-records'] as total_records\n",
        "    FROM local.db.telecom_metrics.snapshots \n",
        "    ORDER BY committed_at\n",
        "\"\"\")\n",
        "updated_snapshots.show(truncate=False)\n",
        "\n",
        "# 4. Time Travel Queries\n",
        "print(\"\\nüîç Time Travel Analysis:\")\n",
        "\n",
        "if len(snapshots) >= 2:\n",
        "    # Compare performance between different snapshots\n",
        "    snapshot_1 = snapshots[0]['snapshot_id']\n",
        "    snapshot_2 = snapshots[-1]['snapshot_id'] if len(snapshots) > 1 else snapshots[0]['snapshot_id']\n",
        "    \n",
        "    print(f\"Comparing snapshots: {snapshot_1} vs Current\")\n",
        "    \n",
        "    # Historical performance\n",
        "    historical_perf = spark.sql(f\"\"\"\n",
        "        SELECT \n",
        "            'Historical' as period,\n",
        "            COUNT(*) as records,\n",
        "            ROUND(AVG(rssi_dbm), 2) as avg_rssi,\n",
        "            ROUND(AVG(latency_ms), 2) as avg_latency,\n",
        "            ROUND(AVG(drop_rate_percent), 2) as avg_drop_rate\n",
        "        FROM local.db.telecom_metrics \n",
        "        FOR SYSTEM_VERSION AS OF {snapshot_1}\n",
        "    \"\"\")\n",
        "    \n",
        "    # Current performance\n",
        "    current_perf = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            'Current' as period,\n",
        "            COUNT(*) as records,\n",
        "            ROUND(AVG(rssi_dbm), 2) as avg_rssi,\n",
        "            ROUND(AVG(latency_ms), 2) as avg_latency,\n",
        "            ROUND(AVG(drop_rate_percent), 2) as avg_drop_rate\n",
        "        FROM local.db.telecom_metrics\n",
        "    \"\"\")\n",
        "    \n",
        "    # Union results for comparison\n",
        "    comparison_df = historical_perf.union(current_perf)\n",
        "    comparison_df.show()\n",
        "    \n",
        "    # Time-based analysis\n",
        "    print(\"\\nüìä Performance Evolution by Technology:\")\n",
        "    spark.sql(f\"\"\"\n",
        "        SELECT \n",
        "            s.technology,\n",
        "            'Historical' as period,\n",
        "            ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "            ROUND(AVG(m.latency_ms), 2) as avg_latency\n",
        "        FROM local.db.telecom_sites s\n",
        "        JOIN (\n",
        "            SELECT * FROM local.db.telecom_metrics \n",
        "            FOR SYSTEM_VERSION AS OF {snapshot_1}\n",
        "        ) m ON s.site_id = m.site_id\n",
        "        GROUP BY s.technology\n",
        "        \n",
        "        UNION ALL\n",
        "        \n",
        "        SELECT \n",
        "            s.technology,\n",
        "            'Current' as period,\n",
        "            ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "            ROUND(AVG(m.latency_ms), 2) as avg_latency\n",
        "        FROM local.db.telecom_sites s\n",
        "        JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "        GROUP BY s.technology\n",
        "        \n",
        "        ORDER BY technology, period\n",
        "    \"\"\").show()\n",
        "\n",
        "# 5. Rollback Simulation\n",
        "print(\"\\n‚è™ Rollback Simulation:\")\n",
        "try:\n",
        "    # Show how to rollback to a previous snapshot\n",
        "    if len(snapshots) > 0:\n",
        "        rollback_snapshot = snapshots[0]['snapshot_id']\n",
        "        print(f\"   To rollback to snapshot {rollback_snapshot}, use:\")\n",
        "        print(f\"   CALL local.system.rollback_to_snapshot('local.db.telecom_metrics', {rollback_snapshot})\")\n",
        "        print(\"   ‚ö†Ô∏è Not executing rollback in demo to preserve data\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Rollback procedure: {str(e)}\")\n",
        "\n",
        "print(\"\\n‚úÖ Time travel and snapshot analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Advanced Schema Evolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Schema Evolution - Linux Enterprise\n",
        "print(\"üîÑ Advanced Schema Evolution (Linux Enterprise)\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# 1. Current Schema Analysis\n",
        "print(\"\\nüìã Current Schema Structure:\")\n",
        "current_schema = spark.sql(\"DESCRIBE local.db.telecom_metrics\")\n",
        "current_schema.show()\n",
        "\n",
        "# Store current columns for comparison\n",
        "current_columns = [row['col_name'] for row in current_schema.collect() if row['col_name'] not in ['', '# Partitioning', '# Metadata']]\n",
        "print(f\"Current columns: {len(current_columns)}\")\n",
        "\n",
        "# 2. Enterprise Schema Evolution - Add Multiple Columns\n",
        "print(\"\\n‚ûï Adding Enterprise Telecom Columns...\")\n",
        "\n",
        "# Add network quality metrics\n",
        "evolution_steps = [\n",
        "    (\"network_quality_score\", \"DOUBLE\", \"Overall network quality score (0-100)\"),\n",
        "    (\"signal_to_noise_ratio\", \"DOUBLE\", \"Signal-to-noise ratio in dB\"),\n",
        "    (\"throughput_mbps\", \"DOUBLE\", \"Actual throughput in Mbps\"),\n",
        "    (\"jitter_ms\", \"DOUBLE\", \"Network jitter in milliseconds\"),\n",
        "    (\"packet_loss_percent\", \"DOUBLE\", \"Packet loss percentage\"),\n",
        "    (\"is_5g_compatible\", \"BOOLEAN\", \"5G compatibility flag\"),\n",
        "    (\"energy_efficiency_score\", \"DOUBLE\", \"Energy efficiency score (0-100)\"),\n",
        "    (\"maintenance_required\", \"BOOLEAN\", \"Maintenance required flag\"),\n",
        "    (\"last_maintenance_date\", \"DATE\", \"Last maintenance date\"),\n",
        "    (\"firmware_version\", \"STRING\", \"Equipment firmware version\"),\n",
        "    (\"antenna_type\", \"STRING\", \"Antenna type classification\"),\n",
        "    (\"coverage_radius_km\", \"DOUBLE\", \"Coverage radius in kilometers\")\n",
        "]\n",
        "\n",
        "for col_name, col_type, description in evolution_steps:\n",
        "    try:\n",
        "        spark.sql(f\"ALTER TABLE local.db.telecom_metrics ADD COLUMN {col_name} {col_type}\").collect()\n",
        "        print(f\"   ‚úÖ Added {col_name} ({col_type}) - {description}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Failed to add {col_name}: {str(e)}\")\n",
        "\n",
        "# 3. Show evolved schema\n",
        "print(\"\\nüìã Evolved Schema Structure:\")\n",
        "evolved_schema = spark.sql(\"DESCRIBE local.db.telecom_metrics\")\n",
        "evolved_schema.show()\n",
        "\n",
        "# 4. Insert Enhanced Enterprise Data\n",
        "print(\"\\nüìä Inserting Enhanced Enterprise Data...\")\n",
        "from pyspark.sql.functions import lit, when, col\n",
        "from datetime import date\n",
        "\n",
        "enhanced_enterprise_records = []\n",
        "current_time = datetime.now()\n",
        "\n",
        "for i, site in enumerate(sites[:100]):  # Use 100 sites for comprehensive demo\n",
        "    # Calculate advanced metrics\n",
        "    rssi = np.random.normal(loc=-72, scale=4)\n",
        "    latency = np.random.normal(loc=35, scale=10)\n",
        "    \n",
        "    # Network quality score algorithm\n",
        "    quality_score = max(0, min(100, 100 - abs(rssi + 50) * 1.5 - latency * 0.5))\n",
        "    \n",
        "    # Signal-to-noise ratio\n",
        "    snr = np.random.normal(loc=15, scale=3)\n",
        "    \n",
        "    # Throughput based on technology and quality\n",
        "    base_throughput = 200 if site[\"technology\"] == \"8G\" else (150 if site[\"technology\"] == \"7G\" else 100)\n",
        "    throughput = np.random.normal(loc=base_throughput, scale=20)\n",
        "    \n",
        "    # Jitter and packet loss\n",
        "    jitter = np.random.exponential(scale=2)\n",
        "    packet_loss = np.random.beta(1, 200) * 100\n",
        "    \n",
        "    # 5G compatibility\n",
        "    is_5g = site[\"technology\"] in [\"7G\", \"8G\"] and site[\"vendor\"] in [\"Ericia\", \"Noson\", \"Samsong\"]\n",
        "    \n",
        "    # Energy efficiency\n",
        "    energy_score = np.random.normal(loc=75, scale=10)\n",
        "    energy_score = max(0, min(100, energy_score))\n",
        "    \n",
        "    # Maintenance flags\n",
        "    maintenance_required = np.random.choice([True, False], p=[0.15, 0.85])\n",
        "    last_maintenance = date(2023, np.random.randint(1, 13), np.random.randint(1, 28))\n",
        "    \n",
        "    # Equipment details\n",
        "    firmware_versions = [\"v2.1.4\", \"v2.2.1\", \"v2.3.0\", \"v3.0.1\"]\n",
        "    firmware = np.random.choice(firmware_versions)\n",
        "    \n",
        "    antenna_types = [\"Omnidirectional\", \"Directional\", \"Sector\", \"Parabolic\"]\n",
        "    antenna = np.random.choice(antenna_types)\n",
        "    \n",
        "    # Coverage radius\n",
        "    coverage_radius = np.random.normal(loc=2.5, scale=0.5)\n",
        "    coverage_radius = max(0.5, coverage_radius)\n",
        "    \n",
        "    enhanced_enterprise_records.append({\n",
        "        \"timestamp\": current_time,\n",
        "        \"region\": site[\"region\"],\n",
        "        \"city\": site[\"city\"],\n",
        "        \"site_id\": site[\"site_id\"],\n",
        "        \"technology\": site[\"technology\"],\n",
        "        \"vendor\": site[\"vendor\"],\n",
        "        \"rssi_dbm\": round(rssi, 2),\n",
        "        \"latency_ms\": round(latency, 2),\n",
        "        \"data_volume_mb\": round(np.random.exponential(scale=8), 2),\n",
        "        \"drop_rate_percent\": round(np.random.beta(1, 200) * 100, 2),\n",
        "        \"cpu_usage_percent\": round(np.clip(np.random.normal(loc=50, scale=10), 0, 100), 2),\n",
        "        \"network_quality_score\": round(quality_score, 2),\n",
        "        \"signal_to_noise_ratio\": round(snr, 2),\n",
        "        \"throughput_mbps\": round(throughput, 2),\n",
        "        \"jitter_ms\": round(jitter, 2),\n",
        "        \"packet_loss_percent\": round(packet_loss, 2),\n",
        "        \"is_5g_compatible\": is_5g,\n",
        "        \"energy_efficiency_score\": round(energy_score, 2),\n",
        "        \"maintenance_required\": maintenance_required,\n",
        "        \"last_maintenance_date\": last_maintenance,\n",
        "        \"firmware_version\": firmware,\n",
        "        \"antenna_type\": antenna,\n",
        "        \"coverage_radius_km\": round(coverage_radius, 2)\n",
        "    })\n",
        "\n",
        "enhanced_enterprise_df = spark.createDataFrame(enhanced_enterprise_records)\n",
        "enhanced_enterprise_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"‚úÖ Enhanced enterprise data inserted!\")\n",
        "\n",
        "# 5. Advanced Analytics with New Schema\n",
        "print(\"\\nüìä Advanced Enterprise Analytics:\")\n",
        "\n",
        "# Network Quality Analysis\n",
        "print(\"\\nüèÜ Network Quality Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.vendor,\n",
        "        s.technology,\n",
        "        COUNT(*) as measurements,\n",
        "        ROUND(AVG(m.network_quality_score), 2) as avg_quality,\n",
        "        ROUND(AVG(m.throughput_mbps), 2) as avg_throughput,\n",
        "        ROUND(AVG(m.energy_efficiency_score), 2) as avg_energy_efficiency,\n",
        "        ROUND(AVG(m.signal_to_noise_ratio), 2) as avg_snr,\n",
        "        SUM(CASE WHEN m.is_5g_compatible THEN 1 ELSE 0 END) as compatible_5g_count\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    WHERE m.network_quality_score IS NOT NULL\n",
        "    GROUP BY s.vendor, s.technology\n",
        "    ORDER BY avg_quality DESC, avg_throughput DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# Maintenance Analysis\n",
        "print(\"\\nüîß Maintenance Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.region,\n",
        "        COUNT(*) as total_sites,\n",
        "        SUM(CASE WHEN m.maintenance_required THEN 1 ELSE 0 END) as maintenance_needed,\n",
        "        ROUND(AVG(m.energy_efficiency_score), 2) as avg_energy_score,\n",
        "        COUNT(DISTINCT m.firmware_version) as firmware_versions\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    WHERE m.maintenance_required IS NOT NULL\n",
        "    GROUP BY s.region\n",
        "    ORDER BY maintenance_needed DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# Technology Performance Comparison\n",
        "print(\"\\nüì° Technology Performance Matrix:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.technology,\n",
        "        COUNT(DISTINCT s.site_id) as sites,\n",
        "        ROUND(AVG(m.throughput_mbps), 2) as avg_throughput,\n",
        "        ROUND(AVG(m.network_quality_score), 2) as avg_quality,\n",
        "        ROUND(AVG(m.jitter_ms), 2) as avg_jitter,\n",
        "        ROUND(AVG(m.packet_loss_percent), 2) as avg_packet_loss,\n",
        "        ROUND(AVG(m.coverage_radius_km), 2) as avg_coverage_radius\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    WHERE m.throughput_mbps IS NOT NULL\n",
        "    GROUP BY s.technology\n",
        "    ORDER BY avg_throughput DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# 6. Schema History and Versioning\n",
        "print(\"\\nüìú Schema Evolution History:\")\n",
        "try:\n",
        "    schema_history = spark.sql(\"SELECT * FROM local.db.telecom_metrics.history ORDER BY made_current_at DESC LIMIT 5\")\n",
        "    schema_history.show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Schema history not available: {str(e)}\")\n",
        "\n",
        "# 7. Column Statistics\n",
        "print(\"\\nüìà Column Statistics:\")\n",
        "new_columns = [col for col in enhanced_enterprise_df.columns if col not in current_columns]\n",
        "print(f\"Added {len(new_columns)} new columns: {', '.join(new_columns)}\")\n",
        "\n",
        "# Show data completeness\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        'network_quality_score' as column_name,\n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(network_quality_score) as non_null_records,\n",
        "        ROUND(COUNT(network_quality_score) * 100.0 / COUNT(*), 2) as completeness_percent\n",
        "    FROM local.db.telecom_metrics\n",
        "    \n",
        "    UNION ALL\n",
        "    \n",
        "    SELECT \n",
        "        'throughput_mbps' as column_name,\n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(throughput_mbps) as non_null_records,\n",
        "        ROUND(COUNT(throughput_mbps) * 100.0 / COUNT(*), 2) as completeness_percent\n",
        "    FROM local.db.telecom_metrics\n",
        "    \n",
        "    UNION ALL\n",
        "    \n",
        "    SELECT \n",
        "        'is_5g_compatible' as column_name,\n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(is_5g_compatible) as non_null_records,\n",
        "        ROUND(COUNT(is_5g_compatible) * 100.0 / COUNT(*), 2) as completeness_percent\n",
        "    FROM local.db.telecom_metrics\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"\\n‚úÖ Advanced schema evolution completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
