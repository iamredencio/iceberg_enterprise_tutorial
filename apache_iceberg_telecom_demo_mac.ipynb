{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Apache Iceberg for Telecom Enterprises (macOS)\n",
        "## A Complete Guide with PySpark & Telecom Time Series Data\n",
        "\n",
        "This notebook demonstrates how to use Apache Iceberg in telecom enterprise environments using PySpark on **macOS**. \n",
        "\n",
        "### Key Features:\n",
        "- **ACID transactions** for reliable telecom data operations\n",
        "- **Schema evolution** for adapting to new network technologies\n",
        "- **Time travel** for historical network performance analysis\n",
        "- **Hidden partitioning** for optimal time-series data queries\n",
        "- **Data compaction** for efficient storage of large telecom datasets\n",
        "\n",
        "### macOS Requirements:\n",
        "- Java 8+ (install via `brew install openjdk@8`)\n",
        "- Python 3.8+\n",
        "- Sufficient disk space for demo data\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. macOS Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"ðŸŽ macOS Apache Iceberg Setup\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check for Java installation on macOS\n",
        "try:\n",
        "    java_version = subprocess.run(['java', '-version'], capture_output=True, text=True, stderr=subprocess.STDOUT)\n",
        "    if java_version.returncode == 0:\n",
        "        print(\"â˜• Java is installed\")\n",
        "        # Set JAVA_HOME using macOS java_home utility\n",
        "        java_home = subprocess.run(['/usr/libexec/java_home'], capture_output=True, text=True)\n",
        "        if java_home.returncode == 0:\n",
        "            os.environ[\"JAVA_HOME\"] = java_home.stdout.strip()\n",
        "            print(f\"ðŸ  JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Could not determine JAVA_HOME\")\n",
        "    else:\n",
        "        print(\"âŒ Java not found!\")\n",
        "        print(\"ðŸ’¡ Install Java using: brew install openjdk@8\")\n",
        "        print(\"   Then run: echo 'export PATH=\\\"/opt/homebrew/opt/openjdk@8/bin:$PATH\\\"' >> ~/.zshrc\")\n",
        "        sys.exit(1)\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ Java not found!\")\n",
        "    print(\"ðŸ’¡ Install Java using: brew install openjdk@8\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"âœ… Java setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python packages for macOS\n",
        "print(\"ðŸ“¦ Installing Python packages for macOS...\")\n",
        "\n",
        "%pip install -q pyspark==3.4.1\n",
        "%pip install -q pyiceberg[s3fs]==0.5.1  \n",
        "%pip install -q pandas>=2.0.0\n",
        "%pip install -q numpy>=1.21.0\n",
        "%pip install -q matplotlib seaborn\n",
        "\n",
        "print(\"âœ… Package installation completed!\")\n",
        "\n",
        "# Test imports\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pyspark\n",
        "    print(f\"ðŸ“Š Pandas: {pd.__version__}\")\n",
        "    print(f\"ðŸ”¢ NumPy: {np.__version__}\")\n",
        "    print(f\"âš¡ PySpark: {pyspark.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "    \n",
        "print(\"ðŸš€ macOS setup completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Spark Configuration for macOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, sum, avg, count\n",
        "import os\n",
        "\n",
        "# Download Iceberg JAR for macOS\n",
        "jar_url = \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar\"\n",
        "jar_path = \"./iceberg-spark-runtime.jar\"\n",
        "warehouse_path = \"./iceberg-warehouse\"\n",
        "\n",
        "print(\"ðŸ“¥ Downloading Iceberg JAR...\")\n",
        "!wget -q {jar_url} -O {jar_path}\n",
        "\n",
        "# Configure Spark with Iceberg for macOS\n",
        "print(\"âš¡ Initializing Spark with Iceberg on macOS...\")\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Iceberg Telecom Demo - macOS\") \\\n",
        "    .config(\"spark.jars\", jar_path) \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .config(\"spark.sql.catalog.local.warehouse\", warehouse_path) \\\n",
        "    .config(\"spark.sql.warehouse.dir\", warehouse_path) \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"âœ… Spark {spark.version} with Iceberg initialized!\")\n",
        "print(f\"ðŸ“ Warehouse: {warehouse_path}\")\n",
        "print(f\"â˜• Java Home: {os.environ.get('JAVA_HOME', 'Not set')}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Generate Telecom Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Telecom data generation for macOS\n",
        "print(\"ðŸ“¡ Generating Telecom Data for macOS...\")\n",
        "\n",
        "# Configuration\n",
        "chunk_size_seconds = 60\n",
        "site_count = 100\n",
        "start_time = datetime(2023, 1, 1)\n",
        "demo_chunks = 50\n",
        "\n",
        "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
        "cities = {\n",
        "    \"North\": [\"Dendam\", \"Rondburg\"],\n",
        "    \"South\": [\"Schieveste\"],\n",
        "    \"East\": [\"Schipstad\", \"Dort\"],\n",
        "    \"West\": [\"Damstad\"],\n",
        "}\n",
        "technologies = [\"6G\", \"7G\"]\n",
        "vendors = [\"Ericia\", \"Noson\", \"Weihu\"]\n",
        "\n",
        "# Generate site metadata\n",
        "sites = []\n",
        "for i in range(site_count):\n",
        "    region = random.choice(regions)\n",
        "    city = random.choice(cities[region])\n",
        "    tech = random.choices(technologies, weights=[0.5, 0.5])[0]\n",
        "    vendor = random.choices(vendors, weights=[0.4, 0.4, 0.2])[0]\n",
        "    site_id = f\"SITE_{i:05d}\"\n",
        "    sites.append({\n",
        "        \"site_id\": site_id,\n",
        "        \"region\": region,\n",
        "        \"city\": city,\n",
        "        \"technology\": tech,\n",
        "        \"vendor\": vendor,\n",
        "    })\n",
        "\n",
        "def generate_telecom_data_chunk(second_offset):\n",
        "    timestamp = start_time + timedelta(seconds=second_offset)\n",
        "    records = []\n",
        "    for site in sites:\n",
        "        region = site[\"region\"]\n",
        "        city = site[\"city\"]\n",
        "        tech = site[\"technology\"]\n",
        "        vendor = site[\"vendor\"]\n",
        "\n",
        "        # Signal strength (RSSI)\n",
        "        base_rssi = -75 if tech == \"7G\" else -85\n",
        "        if vendor == \"Noson\":\n",
        "            base_rssi += 2\n",
        "        if vendor == \"Weihu\":\n",
        "            base_rssi -= 3\n",
        "        rssi = np.random.normal(loc=base_rssi, scale=3)\n",
        "\n",
        "        # Latency\n",
        "        if region == \"West\" or city in [\"Damstad\"]:\n",
        "            base_latency = 80\n",
        "        else:\n",
        "            base_latency = 35 if tech == \"7G\" else 60\n",
        "        latency = np.random.normal(loc=base_latency, scale=8)\n",
        "\n",
        "        # Data volume (in MB)\n",
        "        data_volume = np.random.exponential(scale=6)\n",
        "\n",
        "        # CPU usage\n",
        "        base_cpu = 48 + (7 if tech == \"7G\" else 0) + (5 if vendor == \"Weihu\" else 0)\n",
        "        cpu_usage = np.clip(np.random.normal(loc=base_cpu, scale=9), 0, 100)\n",
        "\n",
        "        # Drop rate - use built-in min function\n",
        "        city_penalty = 0.04 if city in [\"Damstad\", \"Schieveste\"] else 0.01\n",
        "        drop_rate = (\n",
        "            min(1.0, 0.0012 * cpu_usage + city_penalty + np.random.beta(1, 180)) * 100\n",
        "        )\n",
        "\n",
        "        records.append({\n",
        "            \"timestamp\": timestamp,\n",
        "            \"region\": region,\n",
        "            \"city\": city,\n",
        "            \"site_id\": site[\"site_id\"],\n",
        "            \"technology\": tech,\n",
        "            \"vendor\": vendor,\n",
        "            \"rssi_dbm\": round(rssi, 2),\n",
        "            \"latency_ms\": round(latency, 2),\n",
        "            \"data_volume_mb\": round(data_volume, 2),\n",
        "            \"drop_rate_percent\": round(drop_rate, 2),\n",
        "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
        "        })\n",
        "    return records\n",
        "\n",
        "# Generate telecom time series data\n",
        "print(\"âš¡ Generating telecom metrics...\")\n",
        "all_records = []\n",
        "for chunk_idx in range(demo_chunks):\n",
        "    chunk_records = []\n",
        "    for second in range(chunk_size_seconds):\n",
        "        minute_offset = chunk_idx * chunk_size_seconds + second\n",
        "        chunk_records.extend(generate_telecom_data_chunk(minute_offset))\n",
        "    all_records.extend(chunk_records)\n",
        "\n",
        "# Convert to Spark DataFrames\n",
        "telecom_metrics_df = spark.createDataFrame(all_records)\n",
        "sites_df = spark.createDataFrame(sites)\n",
        "\n",
        "print(\"âœ… Telecom data generated!\")\n",
        "print(f\"   ðŸ“¡ Sites: {sites_df.count():,}\")\n",
        "print(f\"   ðŸ“Š Metrics: {telecom_metrics_df.count():,}\")\n",
        "\n",
        "# Show sample data\n",
        "telecom_metrics_df.show(5)\n",
        "sites_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Iceberg Tables for macOS\n",
        "print(\"ðŸ—ï¸ Creating Iceberg tables...\")\n",
        "\n",
        "# Create telecom sites table\n",
        "sites_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"local.db.telecom_sites\")\n",
        "\n",
        "# Create telecom metrics table with timestamp partitioning\n",
        "telecom_metrics_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"timestamp\") \\\n",
        "    .saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"âœ… Iceberg tables created!\")\n",
        "\n",
        "# Verify tables\n",
        "spark.sql(\"SHOW TABLES IN local.db\").show()\n",
        "\n",
        "# Basic analytics\n",
        "print(\"\\nðŸ“Š Telecom Analytics:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.region,\n",
        "        s.technology,\n",
        "        COUNT(DISTINCT s.site_id) as sites,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.region, s.technology\n",
        "    ORDER BY s.region\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"âœ… macOS Iceberg Demo Completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Basic Iceberg Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Iceberg Operations for Telecom Data\n",
        "print(\"ðŸ”§ Basic Iceberg Operations\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Table Information\n",
        "print(\"\\nðŸ“‹ Table Information:\")\n",
        "spark.sql(\"DESCRIBE EXTENDED local.db.telecom_metrics\").show(truncate=False)\n",
        "\n",
        "# 2. Show table properties\n",
        "print(\"\\nâš™ï¸ Table Properties:\")\n",
        "spark.sql(\"SHOW TBLPROPERTIES local.db.telecom_metrics\").show()\n",
        "\n",
        "# 3. Count records by partition (timestamp)\n",
        "print(\"\\nðŸ“Š Records by Time Period:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        DATE(timestamp) as date,\n",
        "        COUNT(*) as records,\n",
        "        COUNT(DISTINCT site_id) as unique_sites\n",
        "    FROM local.db.telecom_metrics \n",
        "    GROUP BY DATE(timestamp)\n",
        "    ORDER BY date\n",
        "    LIMIT 10\n",
        "\"\"\").show()\n",
        "\n",
        "# 4. Basic filtering and aggregation\n",
        "print(\"\\nðŸ” High-Performance Sites (RSSI > -70 dBm):\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.site_id,\n",
        "        s.region,\n",
        "        s.vendor,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    WHERE m.rssi_dbm > -70\n",
        "    GROUP BY s.site_id, s.region, s.vendor\n",
        "    ORDER BY avg_rssi DESC\n",
        "    LIMIT 10\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"âœ… Basic operations completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Time Travel & Snapshots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Travel and Snapshots for Telecom Analysis\n",
        "print(\"ðŸ• Time Travel & Snapshots\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Show all snapshots\n",
        "print(\"\\nðŸ“¸ Available Snapshots:\")\n",
        "snapshots_df = spark.sql(\"SELECT snapshot_id, committed_at, summary FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\")\n",
        "snapshots_df.show(truncate=False)\n",
        "\n",
        "# Store snapshot info for later use\n",
        "snapshots = snapshots_df.collect()\n",
        "print(f\"Total snapshots: {len(snapshots)}\")\n",
        "\n",
        "# 2. Add some new data to create a new snapshot\n",
        "print(\"\\nâž• Adding new telecom data...\")\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Create new data with current timestamp\n",
        "new_timestamp = datetime.now()\n",
        "new_records = []\n",
        "for site in sites[:10]:  # Just first 10 sites for demo\n",
        "    new_records.append({\n",
        "        \"timestamp\": new_timestamp,\n",
        "        \"region\": site[\"region\"],\n",
        "        \"city\": site[\"city\"],\n",
        "        \"site_id\": site[\"site_id\"],\n",
        "        \"technology\": site[\"technology\"],\n",
        "        \"vendor\": site[\"vendor\"],\n",
        "        \"rssi_dbm\": round(np.random.normal(loc=-75, scale=3), 2),\n",
        "        \"latency_ms\": round(np.random.normal(loc=40, scale=8), 2),\n",
        "        \"data_volume_mb\": round(np.random.exponential(scale=6), 2),\n",
        "        \"drop_rate_percent\": round(np.random.beta(1, 200) * 100, 2),\n",
        "        \"cpu_usage_percent\": round(np.clip(np.random.normal(loc=50, scale=10), 0, 100), 2),\n",
        "    })\n",
        "\n",
        "new_data_df = spark.createDataFrame(new_records)\n",
        "new_data_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"âœ… New data added!\")\n",
        "\n",
        "# 3. Show updated snapshots\n",
        "print(\"\\nðŸ“¸ Updated Snapshots:\")\n",
        "updated_snapshots = spark.sql(\"SELECT snapshot_id, committed_at, summary FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\")\n",
        "updated_snapshots.show(truncate=False)\n",
        "\n",
        "# 4. Time travel query - compare current vs previous snapshot\n",
        "if len(snapshots) > 0:\n",
        "    previous_snapshot_id = snapshots[0]['snapshot_id']\n",
        "    \n",
        "    print(f\"\\nðŸ” Comparing data between snapshots:\")\n",
        "    print(f\"Previous snapshot: {previous_snapshot_id}\")\n",
        "    \n",
        "    # Query previous snapshot\n",
        "    previous_count = spark.sql(f\"\"\"\n",
        "        SELECT COUNT(*) as count \n",
        "        FROM local.db.telecom_metrics \n",
        "        FOR SYSTEM_VERSION AS OF {previous_snapshot_id}\n",
        "    \"\"\").collect()[0]['count']\n",
        "    \n",
        "    # Query current data\n",
        "    current_count = spark.sql(\"SELECT COUNT(*) as count FROM local.db.telecom_metrics\").collect()[0]['count']\n",
        "    \n",
        "    print(f\"Previous snapshot records: {previous_count:,}\")\n",
        "    print(f\"Current records: {current_count:,}\")\n",
        "    print(f\"New records added: {current_count - previous_count:,}\")\n",
        "\n",
        "print(\"\\nâœ… Time travel operations completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Schema Evolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema Evolution for Telecom Networks\n",
        "print(\"ðŸ”„ Schema Evolution\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Show current schema\n",
        "print(\"\\nðŸ“‹ Current Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.telecom_metrics\").show()\n",
        "\n",
        "# 2. Add new columns for enhanced telecom monitoring\n",
        "print(\"\\nâž• Adding new telecom monitoring columns...\")\n",
        "\n",
        "# Add network quality score column\n",
        "spark.sql(\"ALTER TABLE local.db.telecom_metrics ADD COLUMN network_quality_score DOUBLE\").collect()\n",
        "print(\"   âœ… Added network_quality_score column\")\n",
        "\n",
        "# Add 5G compatibility flag\n",
        "spark.sql(\"ALTER TABLE local.db.telecom_metrics ADD COLUMN is_5g_compatible BOOLEAN\").collect()\n",
        "print(\"   âœ… Added is_5g_compatible column\")\n",
        "\n",
        "# Add throughput measurement\n",
        "spark.sql(\"ALTER TABLE local.db.telecom_metrics ADD COLUMN throughput_mbps DOUBLE\").collect()\n",
        "print(\"   âœ… Added throughput_mbps column\")\n",
        "\n",
        "# 3. Show updated schema\n",
        "print(\"\\nðŸ“‹ Updated Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.telecom_metrics\").show()\n",
        "\n",
        "# 4. Insert data with new schema\n",
        "print(\"\\nðŸ“Š Inserting data with new schema...\")\n",
        "from pyspark.sql.functions import when, col, lit\n",
        "\n",
        "# Create enhanced data with new columns\n",
        "enhanced_records = []\n",
        "for site in sites[:20]:  # Use first 20 sites\n",
        "    # Calculate network quality score based on RSSI and latency\n",
        "    rssi = np.random.normal(loc=-75, scale=3)\n",
        "    latency = np.random.normal(loc=40, scale=8)\n",
        "    \n",
        "    # Quality score algorithm (0-100)\n",
        "    quality_score = max(0, min(100, 100 - abs(rssi + 50) * 2 - latency))\n",
        "    \n",
        "    # 5G compatibility based on technology and vendor\n",
        "    is_5g = site[\"technology\"] == \"7G\" and site[\"vendor\"] in [\"Ericia\", \"Noson\"]\n",
        "    \n",
        "    # Throughput based on technology and quality\n",
        "    throughput = np.random.normal(\n",
        "        loc=150 if site[\"technology\"] == \"7G\" else 100, \n",
        "        scale=20\n",
        "    )\n",
        "    \n",
        "    enhanced_records.append({\n",
        "        \"timestamp\": datetime.now(),\n",
        "        \"region\": site[\"region\"],\n",
        "        \"city\": site[\"city\"],\n",
        "        \"site_id\": site[\"site_id\"],\n",
        "        \"technology\": site[\"technology\"],\n",
        "        \"vendor\": site[\"vendor\"],\n",
        "        \"rssi_dbm\": round(rssi, 2),\n",
        "        \"latency_ms\": round(latency, 2),\n",
        "        \"data_volume_mb\": round(np.random.exponential(scale=6), 2),\n",
        "        \"drop_rate_percent\": round(np.random.beta(1, 200) * 100, 2),\n",
        "        \"cpu_usage_percent\": round(np.clip(np.random.normal(loc=50, scale=10), 0, 100), 2),\n",
        "        \"network_quality_score\": round(quality_score, 2),\n",
        "        \"is_5g_compatible\": is_5g,\n",
        "        \"throughput_mbps\": round(throughput, 2)\n",
        "    })\n",
        "\n",
        "enhanced_df = spark.createDataFrame(enhanced_records)\n",
        "enhanced_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.telecom_metrics\")\n",
        "\n",
        "print(\"âœ… Enhanced data inserted!\")\n",
        "\n",
        "# 5. Query with new columns\n",
        "print(\"\\nðŸ“Š Network Quality Analysis:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.vendor,\n",
        "        COUNT(*) as measurements,\n",
        "        ROUND(AVG(m.network_quality_score), 2) as avg_quality_score,\n",
        "        ROUND(AVG(m.throughput_mbps), 2) as avg_throughput,\n",
        "        SUM(CASE WHEN m.is_5g_compatible THEN 1 ELSE 0 END) as compatible_5g_sites\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    WHERE m.network_quality_score IS NOT NULL\n",
        "    GROUP BY s.vendor\n",
        "    ORDER BY avg_quality_score DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# 6. Show schema history\n",
        "print(\"\\nðŸ“œ Schema History:\")\n",
        "try:\n",
        "    spark.sql(\"SELECT * FROM local.db.telecom_metrics.history LIMIT 5\").show(truncate=False)\n",
        "except:\n",
        "    print(\"   Schema history not available in this Iceberg version\")\n",
        "\n",
        "print(\"\\nâœ… Schema evolution completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Advanced Analytics & Best Practices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Analytics & Best Practices for Telecom\n",
        "print(\"ðŸš€ Advanced Analytics & Best Practices\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Performance Optimization - Table Maintenance\n",
        "print(\"\\nâš¡ Performance Optimization:\")\n",
        "\n",
        "# Show table files before optimization\n",
        "files_before = spark.sql(\"SELECT COUNT(*) as file_count FROM local.db.telecom_metrics.files\").collect()[0]['file_count']\n",
        "print(f\"   Files before optimization: {files_before}\")\n",
        "\n",
        "# Optimize table (rewrite small files)\n",
        "try:\n",
        "    spark.sql(\"CALL local.system.rewrite_data_files('local.db.telecom_metrics')\").collect()\n",
        "    print(\"   âœ… Data files optimized\")\n",
        "except Exception as e:\n",
        "    print(f\"   âš ï¸ Optimization not available: {str(e)}\")\n",
        "\n",
        "# 2. Advanced Telecom Analytics\n",
        "print(\"\\nðŸ“Š Advanced Telecom Analytics:\")\n",
        "\n",
        "# Network Performance Trend Analysis\n",
        "print(\"\\nðŸ“ˆ Network Performance Trends:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        DATE(timestamp) as date,\n",
        "        s.technology,\n",
        "        COUNT(DISTINCT s.site_id) as active_sites,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_signal_strength,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
        "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate,\n",
        "        ROUND(SUM(m.data_volume_mb)/1024, 2) as total_data_gb\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY DATE(timestamp), s.technology\n",
        "    ORDER BY date, s.technology\n",
        "    LIMIT 20\n",
        "\"\"\").show()\n",
        "\n",
        "# Vendor Performance Comparison\n",
        "print(\"\\nðŸ¢ Vendor Performance Comparison:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.vendor,\n",
        "        s.technology,\n",
        "        COUNT(DISTINCT s.site_id) as sites,\n",
        "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
        "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
        "        ROUND(AVG(m.cpu_usage_percent), 2) as avg_cpu,\n",
        "        CASE \n",
        "            WHEN AVG(m.rssi_dbm) > -70 AND AVG(m.latency_ms) < 50 THEN 'Excellent'\n",
        "            WHEN AVG(m.rssi_dbm) > -80 AND AVG(m.latency_ms) < 70 THEN 'Good'\n",
        "            ELSE 'Needs Improvement'\n",
        "        END as performance_rating\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    GROUP BY s.vendor, s.technology\n",
        "    ORDER BY avg_rssi DESC, avg_latency ASC\n",
        "\"\"\").show()\n",
        "\n",
        "# 3. Anomaly Detection\n",
        "print(\"\\nðŸš¨ Network Anomaly Detection:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.site_id,\n",
        "        s.region,\n",
        "        s.vendor,\n",
        "        m.timestamp,\n",
        "        m.rssi_dbm,\n",
        "        m.latency_ms,\n",
        "        m.drop_rate_percent,\n",
        "        CASE \n",
        "            WHEN m.rssi_dbm < -90 THEN 'Poor Signal'\n",
        "            WHEN m.latency_ms > 100 THEN 'High Latency'\n",
        "            WHEN m.drop_rate_percent > 5 THEN 'High Drop Rate'\n",
        "            WHEN m.cpu_usage_percent > 90 THEN 'High CPU'\n",
        "            ELSE 'Normal'\n",
        "        END as anomaly_type\n",
        "    FROM local.db.telecom_sites s\n",
        "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
        "    WHERE m.rssi_dbm < -90 \n",
        "       OR m.latency_ms > 100 \n",
        "       OR m.drop_rate_percent > 5 \n",
        "       OR m.cpu_usage_percent > 90\n",
        "    ORDER BY m.timestamp DESC\n",
        "    LIMIT 10\n",
        "\"\"\").show()\n",
        "\n",
        "# 4. Best Practices Summary\n",
        "print(\"\\nðŸ“š Telecom Data Lake Best Practices:\")\n",
        "print(\"\"\"\n",
        "ðŸŽ¯ PARTITIONING STRATEGY:\n",
        "   â€¢ Partition by timestamp (hourly/daily) for time-series queries\n",
        "   â€¢ Consider region partitioning for geographically distributed analysis\n",
        "   â€¢ Avoid over-partitioning (< 1GB per partition is too small)\n",
        "\n",
        "ðŸ“Š SCHEMA DESIGN:\n",
        "   â€¢ Use appropriate data types (DOUBLE for measurements, BOOLEAN for flags)\n",
        "   â€¢ Include metadata columns (site_id, region, vendor) for joins\n",
        "   â€¢ Plan for schema evolution (new KPIs, technologies)\n",
        "\n",
        "âš¡ PERFORMANCE OPTIMIZATION:\n",
        "   â€¢ Regular table maintenance (file compaction)\n",
        "   â€¢ Use column pruning in queries\n",
        "   â€¢ Leverage Iceberg's hidden partitioning\n",
        "   â€¢ Monitor query patterns and optimize accordingly\n",
        "\n",
        "ðŸ”’ DATA GOVERNANCE:\n",
        "   â€¢ Implement time travel for audit trails\n",
        "   â€¢ Use snapshots for backup and recovery\n",
        "   â€¢ Track schema changes for compliance\n",
        "   â€¢ Set up data quality checks\n",
        "\n",
        "ðŸš€ OPERATIONAL EXCELLENCE:\n",
        "   â€¢ Monitor table growth and file sizes\n",
        "   â€¢ Automate maintenance tasks\n",
        "   â€¢ Use appropriate file formats (Parquet recommended)\n",
        "   â€¢ Implement proper access controls\n",
        "\"\"\")\n",
        "\n",
        "# 5. Table Statistics\n",
        "print(\"\\nðŸ“ˆ Final Table Statistics:\")\n",
        "total_records = spark.sql(\"SELECT COUNT(*) as total FROM local.db.telecom_metrics\").collect()[0]['total']\n",
        "total_sites = spark.sql(\"SELECT COUNT(DISTINCT site_id) as sites FROM local.db.telecom_metrics\").collect()[0]['sites']\n",
        "date_range = spark.sql(\"SELECT MIN(timestamp) as min_date, MAX(timestamp) as max_date FROM local.db.telecom_metrics\").collect()[0]\n",
        "\n",
        "print(f\"   ðŸ“Š Total Records: {total_records:,}\")\n",
        "print(f\"   ðŸ“¡ Unique Sites: {total_sites:,}\")\n",
        "print(f\"   ðŸ“… Date Range: {date_range['min_date']} to {date_range['max_date']}\")\n",
        "\n",
        "# Show final snapshots\n",
        "final_snapshots = spark.sql(\"SELECT COUNT(*) as snapshot_count FROM local.db.telecom_metrics.snapshots\").collect()[0]['snapshot_count']\n",
        "print(f\"   ðŸ“¸ Total Snapshots: {final_snapshots}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ macOS Apache Iceberg Telecom Demo Complete!\")\n",
        "print(\"   Ready for enterprise telecom data lake deployment!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
