{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Apache Iceberg for On-Premise Enterprises\n",
        "## A Complete Guide with PySpark\n",
        "\n",
        "This notebook demonstrates how to use Apache Iceberg in on-premise enterprise environments using PySpark. Apache Iceberg is an open table format for huge analytic datasets that provides:\n",
        "\n",
        "- **ACID transactions**\n",
        "- **Schema evolution**\n",
        "- **Time travel**\n",
        "- **Hidden partitioning**\n",
        "- **Data compaction**\n",
        "- **Rollback capabilities**\n",
        "\n",
        "### Why Iceberg for Enterprises?\n",
        "- Reliable data lake operations\n",
        "- Multi-engine compatibility (Spark, Flink, Trino, etc.)\n",
        "- Better performance through advanced optimization\n",
        "- Enterprise-grade data governance\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, let's install the required dependencies for Google Colab environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Java 8 (required for PySpark)\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-8-jdk-headless\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Install PySpark with Iceberg support\n",
        "%pip install pyspark==3.4.1\n",
        "%pip install pyiceberg[s3fs,duckdb]==0.5.1\n",
        "%pip install pandas==2.0.3\n",
        "%pip install matplotlib seaborn\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Spark Configuration with Iceberg\n",
        "\n",
        "Configure Spark to work with Apache Iceberg. In enterprise environments, you would typically configure this in your cluster settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Download Iceberg JAR for Spark\n",
        "!wget -q https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar -O /content/iceberg-spark-runtime.jar\n",
        "\n",
        "# Configure Spark with Iceberg\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Iceberg Enterprise Demo\") \\\n",
        "    .config(\"spark.jars\", \"/content/iceberg-spark-runtime.jar\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .config(\"spark.sql.catalog.local.warehouse\", \"/content/iceberg-warehouse\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/content/iceberg-warehouse\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"‚úÖ Spark {spark.version} with Iceberg initialized successfully!\")\n",
        "print(f\"üìÅ Warehouse location: /content/iceberg-warehouse\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Creating Sample Enterprise Data\n",
        "\n",
        "Let's create sample datasets that represent typical enterprise scenarios:\n",
        "- Customer data\n",
        "- Sales transactions\n",
        "- Product catalog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Generate sample customer data\n",
        "def generate_customer_data(num_customers=1000):\n",
        "    customers = []\n",
        "    for i in range(num_customers):\n",
        "        customers.append({\n",
        "            'customer_id': f'CUST_{i:06d}',\n",
        "            'first_name': f'FirstName{i}',\n",
        "            'last_name': f'LastName{i}',\n",
        "            'email': f'customer{i}@enterprise.com',\n",
        "            'registration_date': datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1400)),\n",
        "            'customer_segment': random.choice(['Premium', 'Standard', 'Basic']),\n",
        "            'credit_limit': random.randint(1000, 50000),\n",
        "            'country': random.choice(['USA', 'Canada', 'UK', 'Germany', 'France']),\n",
        "            'is_active': random.choice([True, False])\n",
        "        })\n",
        "    return customers\n",
        "\n",
        "# Generate sample sales data\n",
        "def generate_sales_data(num_transactions=5000):\n",
        "    sales = []\n",
        "    for i in range(num_transactions):\n",
        "        sales.append({\n",
        "            'transaction_id': f'TXN_{i:08d}',\n",
        "            'customer_id': f'CUST_{random.randint(0, 999):06d}',\n",
        "            'product_id': f'PROD_{random.randint(1, 100):03d}',\n",
        "            'transaction_date': datetime(2023, 1, 1) + timedelta(days=random.randint(0, 365)),\n",
        "            'quantity': random.randint(1, 10),\n",
        "            'unit_price': round(random.uniform(10.0, 500.0), 2),\n",
        "            'discount_percentage': random.uniform(0, 0.3),\n",
        "            'payment_method': random.choice(['Credit Card', 'Debit Card', 'Cash', 'Bank Transfer']),\n",
        "            'sales_rep': f'REP_{random.randint(1, 50):03d}'\n",
        "        })\n",
        "    return sales\n",
        "\n",
        "# Create DataFrames\n",
        "customers_data = generate_customer_data(1000)\n",
        "sales_data = generate_sales_data(5000)\n",
        "\n",
        "customers_df = spark.createDataFrame(customers_data)\n",
        "sales_df = spark.createDataFrame(sales_data)\n",
        "\n",
        "# Add calculated columns\n",
        "sales_df = sales_df.withColumn(\n",
        "    'total_amount', \n",
        "    col('quantity') * col('unit_price') * (1 - col('discount_percentage'))\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Sample data generated:\")\n",
        "print(f\"   üìä Customers: {customers_df.count():,} records\")\n",
        "print(f\"   üí∞ Sales: {sales_df.count():,} transactions\")\n",
        "print(f\"   üíµ Total revenue: ${sales_df.select(sum('total_amount')).collect()[0][0]:,.2f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Creating Iceberg Tables\n",
        "\n",
        "Now let's create Iceberg tables. In enterprise environments, these would typically be stored in distributed storage systems like HDFS, S3, or Azure Data Lake.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create customers Iceberg table\n",
        "customers_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"path\", \"/content/iceberg-warehouse/customers\") \\\n",
        "    .saveAsTable(\"local.db.customers\")\n",
        "\n",
        "# Create sales Iceberg table with partitioning (enterprise best practice)\n",
        "sales_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"path\", \"/content/iceberg-warehouse/sales\") \\\n",
        "    .partitionBy(\"transaction_date\") \\\n",
        "    .saveAsTable(\"local.db.sales\")\n",
        "\n",
        "print(\"‚úÖ Iceberg tables created successfully!\")\n",
        "\n",
        "# Show table information\n",
        "print(\"\\nüìã Table Details:\")\n",
        "spark.sql(\"SHOW TABLES IN local.db\").show()\n",
        "\n",
        "# Show customers table schema\n",
        "print(\"\\nüë• Customers Table Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.customers\").show()\n",
        "\n",
        "# Show sales table schema\n",
        "print(\"\\nüí∞ Sales Table Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.sales\").show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Basic Iceberg Operations\n",
        "\n",
        "Let's explore basic operations that are crucial for enterprise data management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Iceberg tables\n",
        "customers_iceberg = spark.read.format(\"iceberg\").table(\"local.db.customers\")\n",
        "sales_iceberg = spark.read.format(\"iceberg\").table(\"local.db.sales\")\n",
        "\n",
        "print(\"üìä Data Summary:\")\n",
        "print(f\"   Customers: {customers_iceberg.count():,}\")\n",
        "print(f\"   Sales Transactions: {sales_iceberg.count():,}\")\n",
        "\n",
        "# Sample queries\n",
        "print(\"\\nüîç Top 5 Customer Segments by Count:\")\n",
        "customers_iceberg.groupBy(\"customer_segment\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()\n",
        "\n",
        "print(\"\\nüí≥ Sales by Payment Method:\")\n",
        "sales_iceberg.groupBy(\"payment_method\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"transaction_count\"),\n",
        "        round(sum(\"total_amount\"), 2).alias(\"total_revenue\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"total_revenue\").desc()) \\\n",
        "    .show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Time Travel - Enterprise Data Recovery\n",
        "\n",
        "One of Iceberg's most powerful features for enterprises is time travel, allowing you to query data as it existed at any point in time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View table history\n",
        "print(\"üìú Sales Table History:\")\n",
        "spark.sql(\"SELECT * FROM local.db.sales.history\").show(truncate=False)\n",
        "\n",
        "# Get current snapshot info\n",
        "print(\"\\nüì∏ Current Snapshots:\")\n",
        "spark.sql(\"SELECT * FROM local.db.sales.snapshots\").show(truncate=False)\n",
        "\n",
        "# Make some changes to demonstrate time travel\n",
        "print(\"\\nüîÑ Making changes to demonstrate time travel...\")\n",
        "\n",
        "# Add new sales data (simulating new transactions)\n",
        "new_sales_data = generate_sales_data(100)\n",
        "new_sales_df = spark.createDataFrame(new_sales_data)\n",
        "new_sales_df = new_sales_df.withColumn(\n",
        "    'total_amount', \n",
        "    col('quantity') * col('unit_price') * (1 - col('discount_percentage'))\n",
        ")\n",
        "\n",
        "# Append to existing table\n",
        "new_sales_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .saveAsTable(\"local.db.sales\")\n",
        "\n",
        "print(f\"‚úÖ Added {new_sales_df.count()} new transactions\")\n",
        "print(f\"üìä Total transactions now: {spark.read.format('iceberg').table('local.db.sales').count():,}\")\n",
        "\n",
        "# Show updated history\n",
        "print(\"\\nüìú Updated Table History:\")\n",
        "spark.sql(\"SELECT * FROM local.db.sales.history ORDER BY made_current_at\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Schema Evolution - Enterprise Agility\n",
        "\n",
        "Schema evolution allows enterprises to adapt their data structures without breaking existing pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show current schema\n",
        "print(\"üìã Current Customers Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.customers\").show()\n",
        "\n",
        "# Add a new column (common enterprise requirement)\n",
        "print(\"\\nüîß Adding new column: loyalty_points\")\n",
        "spark.sql(\"\"\"\n",
        "    ALTER TABLE local.db.customers \n",
        "    ADD COLUMN loyalty_points INT AFTER credit_limit\n",
        "\"\"\")\n",
        "\n",
        "# Show updated schema\n",
        "print(\"\\nüìã Updated Schema:\")\n",
        "spark.sql(\"DESCRIBE local.db.customers\").show()\n",
        "\n",
        "# Update some records with loyalty points\n",
        "print(\"\\nüìù Updating loyalty points for Premium customers...\")\n",
        "spark.sql(\"\"\"\n",
        "    UPDATE local.db.customers \n",
        "    SET loyalty_points = CAST(credit_limit * 0.1 AS INT)\n",
        "    WHERE customer_segment = 'Premium'\n",
        "\"\"\")\n",
        "\n",
        "# Verify the update\n",
        "print(\"\\n‚úÖ Premium customers with loyalty points:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT customer_segment, \n",
        "           COUNT(*) as customer_count,\n",
        "           AVG(loyalty_points) as avg_loyalty_points\n",
        "    FROM local.db.customers \n",
        "    WHERE customer_segment = 'Premium'\n",
        "    GROUP BY customer_segment\n",
        "\"\"\").show()\n",
        "\n",
        "# Show that old queries still work (backward compatibility)\n",
        "print(\"\\nüîÑ Backward compatibility check - old queries still work:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT customer_segment, COUNT(*) as count\n",
        "    FROM local.db.customers \n",
        "    GROUP BY customer_segment\n",
        "    ORDER BY count DESC\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Enterprise Best Practices Summary\n",
        "\n",
        "Key recommendations for using Apache Iceberg in enterprise environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display best practices and final summary\n",
        "print(\"üè¢ ENTERPRISE APACHE ICEBERG BEST PRACTICES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_practices = [\n",
        "    \"üéØ **Partitioning Strategy**\",\n",
        "    \"   ‚Ä¢ Use date/time partitioning for time-series data\",\n",
        "    \"   ‚Ä¢ Consider business-specific partitions (region, department)\",\n",
        "    \"   ‚Ä¢ Avoid over-partitioning (aim for 100MB+ per partition)\",\n",
        "    \"\",\n",
        "    \"üîß **Table Maintenance**\",\n",
        "    \"   ‚Ä¢ Schedule regular compaction jobs\",\n",
        "    \"   ‚Ä¢ Implement snapshot cleanup policies\",\n",
        "    \"   ‚Ä¢ Monitor table statistics and file counts\",\n",
        "    \"\",\n",
        "    \"üîí **Data Governance**\",\n",
        "    \"   ‚Ä¢ Use schema evolution carefully with proper testing\",\n",
        "    \"   ‚Ä¢ Implement data lineage tracking\",\n",
        "    \"   ‚Ä¢ Set up proper access controls and auditing\",\n",
        "    \"\",\n",
        "    \"üìä **Performance Optimization**\",\n",
        "    \"   ‚Ä¢ Use vectorized readers when available\",\n",
        "    \"   ‚Ä¢ Implement predicate pushdown in queries\",\n",
        "    \"   ‚Ä¢ Optimize file sizes (128MB-1GB per file)\",\n",
        "    \"\",\n",
        "    \"üõ°Ô∏è **Reliability & Recovery**\",\n",
        "    \"   ‚Ä¢ Implement backup strategies for metadata\",\n",
        "    \"   ‚Ä¢ Test disaster recovery procedures\",\n",
        "    \"   ‚Ä¢ Use time travel for audit and compliance\",\n",
        "    \"\",\n",
        "    \"üîó **Integration**\",\n",
        "    \"   ‚Ä¢ Standardize on Iceberg across analytics engines\",\n",
        "    \"   ‚Ä¢ Implement proper CI/CD for schema changes\",\n",
        "    \"   ‚Ä¢ Use catalog services for metadata management\"\n",
        "]\n",
        "\n",
        "for practice in best_practices:\n",
        "    print(practice)\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\\nüìà DEMO SUMMARY\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "summary_stats = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        'Total Customers' as metric,\n",
        "        CAST(COUNT(*) AS STRING) as value\n",
        "    FROM local.db.customers\n",
        "    UNION ALL\n",
        "    SELECT \n",
        "        'Total Sales Transactions',\n",
        "        CAST(COUNT(*) AS STRING)\n",
        "    FROM local.db.sales\n",
        "    UNION ALL\n",
        "    SELECT \n",
        "        'Total Revenue',\n",
        "        CONCAT('$', CAST(ROUND(SUM(total_amount), 2) AS STRING))\n",
        "    FROM local.db.sales\n",
        "    UNION ALL\n",
        "    SELECT \n",
        "        'Active Customers',\n",
        "        CAST(SUM(CASE WHEN is_active THEN 1 ELSE 0 END) AS STRING)\n",
        "    FROM local.db.customers\n",
        "\"\"\")\n",
        "\n",
        "summary_stats.show(truncate=False)\n",
        "\n",
        "print(\"\\n‚úÖ Apache Iceberg Enterprise Demo Completed Successfully!\")\n",
        "print(\"\\nüöÄ Ready for production deployment with proper configuration!\")\n",
        "\n",
        "# Optional cleanup\n",
        "print(\"\\n‚èπÔ∏è Stopping Spark session...\")\n",
        "spark.stop()\n",
        "print(\"‚úÖ Demo completed! Your Iceberg tables are preserved in /content/iceberg-warehouse/\")\n",
        "print(\"üìö To learn more, visit: https://iceberg.apache.org/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
