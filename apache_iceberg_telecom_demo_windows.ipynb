{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Iceberg for Telecom Enterprises (Windows)\n",
    "## A Complete Guide with PySpark & Telecom Time Series Data\n",
    "\n",
    "This notebook demonstrates how to use Apache Iceberg in telecom enterprise environments using PySpark on **Windows**.\n",
    "\n",
    "### Key Features:\n",
    "- **ACID transactions** for reliable telecom data operations\n",
    "- **Schema evolution** for adapting to new network technologies\n",
    "- **Time travel** for historical network performance analysis\n",
    "- **Hidden partitioning** for optimal time-series data queries\n",
    "- **Data compaction** for efficient storage of large telecom datasets\n",
    "\n",
    "### Windows Requirements:\n",
    "- Java 8+ (install via [Oracle JDK](https://www.oracle.com/java/technologies/downloads/) or [OpenJDK](https://adoptium.net/))\n",
    "- Python 3.8+ (install via [Python.org](https://www.python.org/downloads/windows/) or [Anaconda](https://www.anaconda.com/products/distribution))\n",
    "- Windows 10/11 or Windows Server 2016+\n",
    "- PowerShell or Command Prompt access\n",
    "- Sufficient disk space for demo data\n",
    "\n",
    "### Windows-Specific Features:\n",
    "- Automatic Java detection via Windows Registry\n",
    "- PowerShell and CMD compatibility\n",
    "- Windows path handling (backslashes)\n",
    "- Conda/pip environment management\n",
    "- Windows Defender compatibility\n",
    "\n",
    "**Note:** This notebook is optimized specifically for Windows environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Windows Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"🪟 Windows Apache Iceberg Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verify we're on Windows\n",
    "if platform.system() != \"Windows\":\n",
    "    print(\"⚠️ This notebook is optimized for Windows\")\n",
    "    print(\"💡 Consider using the macOS or Linux version for your platform\")\n",
    "\n",
    "print(f\"🖥️ Windows Version: {platform.platform()}\")\n",
    "print(f\"🐍 Python Version: {platform.python_version()}\")\n",
    "\n",
    "# Check for Java installation on Windows\n",
    "def find_java_windows():\n",
    "    \"\"\"Find Java installation on Windows using multiple methods\"\"\"\n",
    "    java_paths = []\n",
    "    \n",
    "    # Method 1: Check JAVA_HOME environment variable\n",
    "    java_home = os.environ.get('JAVA_HOME')\n",
    "    if java_home:\n",
    "        java_exe = os.path.join(java_home, 'bin', 'java.exe')\n",
    "        if os.path.exists(java_exe):\n",
    "            java_paths.append(java_home)\n",
    "    \n",
    "    # Method 2: Check common installation paths\n",
    "    common_paths = [\n",
    "        r\"C:\\Program Files\\Java\",\n",
    "        r\"C:\\Program Files (x86)\\Java\",\n",
    "        r\"C:\\Program Files\\Eclipse Adoptium\",\n",
    "        r\"C:\\Program Files\\Microsoft\\jdk\",\n",
    "        r\"C:\\Program Files\\Zulu\\zulu-8\",\n",
    "    ]\n",
    "    \n",
    "    for base_path in common_paths:\n",
    "        if os.path.exists(base_path):\n",
    "            for item in os.listdir(base_path):\n",
    "                potential_java_home = os.path.join(base_path, item)\n",
    "                java_exe = os.path.join(potential_java_home, 'bin', 'java.exe')\n",
    "                if os.path.exists(java_exe):\n",
    "                    java_paths.append(potential_java_home)\n",
    "    \n",
    "    # Method 3: Try to find java.exe in PATH\n",
    "    try:\n",
    "        result = subprocess.run(['where', 'java'], capture_output=True, text=True, shell=True)\n",
    "        if result.returncode == 0:\n",
    "            java_exe_path = result.stdout.strip().split('\\n')[0]\n",
    "            java_home = os.path.dirname(os.path.dirname(java_exe_path))\n",
    "            java_paths.append(java_home)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return list(set(java_paths))  # Remove duplicates\n",
    "\n",
    "# Find and configure Java\n",
    "print(\"\\n☕ Checking Java installation...\")\n",
    "try:\n",
    "    # Test if java command works\n",
    "    java_version = subprocess.run(['java', '-version'], capture_output=True, text=True, stderr=subprocess.STDOUT, shell=True)\n",
    "    if java_version.returncode == 0:\n",
    "        print(\"✅ Java is accessible via PATH\")\n",
    "        \n",
    "        # Find Java Home\n",
    "        java_homes = find_java_windows()\n",
    "        if java_homes:\n",
    "            # Use the first valid Java installation\n",
    "            selected_java_home = java_homes[0]\n",
    "            os.environ[\"JAVA_HOME\"] = selected_java_home\n",
    "            print(f\"🏠 JAVA_HOME set to: {selected_java_home}\")\n",
    "            \n",
    "            # Show Java version\n",
    "            version_lines = java_version.stdout.split('\\n')\n",
    "            for line in version_lines[:2]:\n",
    "                if line.strip():\n",
    "                    print(f\"📋 {line.strip()}\")\n",
    "        else:\n",
    "            print(\"⚠️ Could not determine JAVA_HOME automatically\")\n",
    "            print(\"💡 Please set JAVA_HOME environment variable manually\")\n",
    "    else:\n",
    "        print(\"❌ Java not found in PATH!\")\n",
    "        print(\"\\n💡 Install Java on Windows:\")\n",
    "        print(\"   Option 1: Download from https://adoptium.net/ (Recommended)\")\n",
    "        print(\"   Option 2: Download from https://www.oracle.com/java/technologies/downloads/\")\n",
    "        print(\"   Option 3: Use Chocolatey: choco install openjdk\")\n",
    "        print(\"   Option 4: Use winget: winget install Microsoft.OpenJDK.11\")\n",
    "        print(\"\\n   After installation, restart your terminal/Jupyter and run this cell again.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Java command not found!\")\n",
    "    print(\"\\n💡 Install Java on Windows:\")\n",
    "    print(\"   1. Download OpenJDK from https://adoptium.net/\")\n",
    "    print(\"   2. Run the installer (it will set up PATH automatically)\")\n",
    "    print(\"   3. Restart your terminal/Jupyter\")\n",
    "    print(\"   4. Run this cell again\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n✅ Java setup completed for Windows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages for Windows\n",
    "print(\"📦 Installing Python packages for Windows...\")\n",
    "print(\"🔄 This may take a few minutes on Windows...\")\n",
    "\n",
    "# Windows-specific pip installation with timeout handling\n",
    "%pip install -q --timeout=300 pyspark==3.4.1\n",
    "%pip install -q --timeout=300 pyiceberg[s3fs]==0.5.1  \n",
    "%pip install -q --timeout=300 pandas>=2.0.0\n",
    "%pip install -q --timeout=300 numpy>=1.21.0\n",
    "%pip install -q --timeout=300 matplotlib seaborn\n",
    "\n",
    "# Windows-specific: Install additional utilities\n",
    "%pip install -q --timeout=300 requests urllib3\n",
    "\n",
    "print(\"✅ Package installation completed!\")\n",
    "\n",
    "# Test imports with Windows-specific error handling\n",
    "print(\"\\n🧪 Testing package imports...\")\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pyspark\n",
    "    import requests\n",
    "    \n",
    "    print(f\"📊 Pandas: {pd.__version__}\")\n",
    "    print(f\"🔢 NumPy: {np.__version__}\")\n",
    "    print(f\"⚡ PySpark: {pyspark.__version__}\")\n",
    "    print(f\"🌐 Requests: {requests.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"💡 Try restarting the kernel and running the cell again\")\n",
    "    print(\"💡 On Windows, some packages may require Visual C++ Build Tools\")\n",
    "    \n",
    "print(\"\\n🚀 Windows package setup completed!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Creating Iceberg Tables for Telecom Data\n",
    "\n",
    "Create Iceberg tables with time-based partitioning for optimal telecom time series queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Iceberg Tables for Windows\n",
    "print(\"🏗️ Creating Iceberg Tables for Windows Environment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create database/namespace in Iceberg\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS local.db\")\n",
    "print(\"✅ Created database namespace\")\n",
    "\n",
    "# Create Sites table (reference data)\n",
    "print(\"\\n🏢 Creating Sites table...\")\n",
    "sites_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .saveAsTable(\"local.db.telecom_sites\")\n",
    "\n",
    "print(\"✅ Sites table created\")\n",
    "\n",
    "# Create Metrics table with time-based partitioning (optimal for Windows)\n",
    "print(\"\\n📊 Creating Metrics table with partitioning...\")\n",
    "telecom_metrics_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .partitionBy(\"timestamp\") \\\n",
    "    .option(\"write.spark.fanout.enabled\", \"true\") \\\n",
    "    .option(\"write.metadata.compression-codec\", \"gzip\") \\\n",
    "    .saveAsTable(\"local.db.telecom_metrics\")\n",
    "\n",
    "print(\"✅ Metrics table created with time partitioning\")\n",
    "\n",
    "# Verify tables\n",
    "print(\"\\n📋 Available Tables:\")\n",
    "spark.sql(\"SHOW TABLES IN local.db\").show()\n",
    "\n",
    "# Initial analytics\n",
    "print(\"\\n📊 Windows Telecom Analytics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        s.vendor,\n",
    "        COUNT(DISTINCT s.site_id) as sites,\n",
    "        ROUND(AVG(m.rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(m.latency_ms), 2) as avg_latency,\n",
    "        ROUND(AVG(m.drop_rate_percent), 2) as avg_drop_rate\n",
    "    FROM local.db.telecom_sites s\n",
    "    JOIN local.db.telecom_metrics m ON s.site_id = m.site_id\n",
    "    GROUP BY s.vendor\n",
    "    ORDER BY avg_rssi DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n🕐 Time Travel Demo:\")\n",
    "snapshots = spark.sql(\"SELECT snapshot_id, committed_at FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\")\n",
    "snapshot_count = snapshots.count()\n",
    "print(f\"📸 Available snapshots: {snapshot_count}\")\n",
    "for row in snapshots.collect():\n",
    "    print(f\"   Snapshot: {row.snapshot_id} at {row.committed_at}\")\n",
    "\n",
    "print(\"\\n✅ Windows Iceberg setup completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Basic Iceberg Operations\n",
    "\n",
    "Explore fundamental Iceberg operations for telecom data management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Iceberg Operations - Windows Optimized\n",
    "print(\"🔧 Basic Iceberg Operations (Windows)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Table Information\n",
    "print(\"\\n📋 Table Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.telecom_metrics\").show()\n",
    "\n",
    "# 2. Table properties (Windows-specific display)\n",
    "print(\"\\n⚙️ Table Properties:\")\n",
    "props = spark.sql(\"SHOW TBLPROPERTIES local.db.telecom_metrics\")\n",
    "props.show(truncate=False)\n",
    "\n",
    "# 3. Quick data exploration\n",
    "print(\"\\n📊 Data Overview:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT site_id) as unique_sites,\n",
    "        MIN(timestamp) as earliest_record,\n",
    "        MAX(timestamp) as latest_record\n",
    "    FROM local.db.telecom_metrics\n",
    "\"\"\").show()\n",
    "\n",
    "# 4. Performance by region (Windows-optimized query)\n",
    "print(\"\\n🌍 Regional Performance Analysis:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        region,\n",
    "        COUNT(*) as measurements,\n",
    "        ROUND(AVG(rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(latency_ms), 2) as avg_latency,\n",
    "        ROUND(AVG(cpu_usage_percent), 2) as avg_cpu\n",
    "    FROM local.db.telecom_metrics\n",
    "    GROUP BY region\n",
    "    ORDER BY avg_rssi DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 5. Technology comparison\n",
    "print(\"\\n📡 Technology Performance:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        technology,\n",
    "        COUNT(*) as measurements,\n",
    "        ROUND(AVG(rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(latency_ms), 2) as avg_latency,\n",
    "        ROUND(MAX(drop_rate_percent), 2) as max_drop_rate\n",
    "    FROM local.db.telecom_metrics\n",
    "    GROUP BY technology\n",
    "    ORDER BY technology\n",
    "\"\"\").show()\n",
    "\n",
    "# 6. Filtering examples\n",
    "print(\"\\n🔍 Filtering High-Performance Sites:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT site_id, region, AVG(rssi_dbm) as avg_rssi\n",
    "    FROM local.db.telecom_metrics\n",
    "    WHERE rssi_dbm > -70\n",
    "    GROUP BY site_id, region\n",
    "    ORDER BY avg_rssi DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"✅ Basic operations completed successfully on Windows!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Time Travel & Snapshots\n",
    "\n",
    "Demonstrate Iceberg's time travel capabilities for historical telecom data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel & Snapshots - Windows Implementation\n",
    "print(\"🕐 Time Travel & Snapshots (Windows)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Add more data to create snapshots\n",
    "print(\"\\n📊 Adding new data for time travel demo...\")\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "additional_records = []\n",
    "for site in sites[:20]:  # Use subset for Windows performance\n",
    "    record = {\n",
    "        \"timestamp\": datetime.now() + timedelta(minutes=random.randint(1, 60)),\n",
    "        \"region\": site[\"region\"],\n",
    "        \"city\": site[\"city\"],\n",
    "        \"site_id\": site[\"site_id\"],\n",
    "        \"technology\": site[\"technology\"],\n",
    "        \"vendor\": site[\"vendor\"],\n",
    "        \"rssi_dbm\": round(random.uniform(-85, -65), 2),\n",
    "        \"latency_ms\": round(random.uniform(25, 75), 2),\n",
    "        \"data_volume_mb\": round(random.uniform(1, 15), 2),\n",
    "        \"drop_rate_percent\": round(random.uniform(0, 2), 2),\n",
    "        \"cpu_usage_percent\": round(random.uniform(30, 80), 2),\n",
    "    }\n",
    "    additional_records.append(record)\n",
    "\n",
    "# Insert additional data\n",
    "additional_df = spark.createDataFrame(additional_records)\n",
    "additional_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .saveAsTable(\"local.db.telecom_metrics\")\n",
    "\n",
    "print(\"✅ Added new data\")\n",
    "\n",
    "# Show snapshots\n",
    "print(\"\\n📸 Available Snapshots:\")\n",
    "snapshots_df = spark.sql(\"SELECT snapshot_id, committed_at FROM local.db.telecom_metrics.snapshots ORDER BY committed_at\")\n",
    "snapshots_df.show()\n",
    "\n",
    "# Compare snapshots if available\n",
    "snapshots = snapshots_df.collect()\n",
    "if len(snapshots) >= 2:\n",
    "    first_snapshot = snapshots[0].snapshot_id\n",
    "    latest_snapshot = snapshots[-1].snapshot_id\n",
    "    \n",
    "    print(f\"\\n🔍 Comparing snapshots on Windows:\")\n",
    "    print(f\"   First: {first_snapshot}\")\n",
    "    print(f\"   Latest: {latest_snapshot}\")\n",
    "    \n",
    "    # Query first snapshot\n",
    "    first_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as record_count \n",
    "        FROM local.db.telecom_metrics \n",
    "        VERSION AS OF {first_snapshot}\n",
    "    \"\"\").collect()[0].record_count\n",
    "    \n",
    "    # Query latest snapshot\n",
    "    latest_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as record_count \n",
    "        FROM local.db.telecom_metrics \n",
    "        VERSION AS OF {latest_snapshot}\n",
    "    \"\"\").collect()[0].record_count\n",
    "    \n",
    "    print(f\"📊 Records in first snapshot: {first_count:,}\")\n",
    "    print(f\"📊 Records in latest snapshot: {latest_count:,}\")\n",
    "    print(f\"📈 Records added: {latest_count - first_count:,}\")\n",
    "\n",
    "print(\"✅ Time travel operations completed on Windows!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Schema Evolution\n",
    "\n",
    "Demonstrate how Iceberg handles schema changes without breaking existing queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Evolution - Windows Implementation\n",
    "print(\"🔄 Schema Evolution (Windows)\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Add new columns for Windows telecom metrics\n",
    "print(\"\\n➕ Adding new columns for enhanced Windows monitoring...\")\n",
    "\n",
    "# Add signal quality category\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.db.telecom_metrics \n",
    "    ADD COLUMN signal_quality_category STRING\n",
    "\"\"\")\n",
    "print(\"   ✅ Added signal_quality_category column\")\n",
    "\n",
    "# Add Windows-specific monitoring metrics\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.db.telecom_metrics \n",
    "    ADD COLUMN windows_compatibility_score DOUBLE\n",
    "\"\"\")\n",
    "print(\"   ✅ Added windows_compatibility_score column\")\n",
    "\n",
    "# Show updated schema\n",
    "print(\"\\n📋 Updated Schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.telecom_metrics\").show()\n",
    "\n",
    "# Insert data with new schema\n",
    "print(\"\\n📊 Inserting data with enhanced Windows metrics...\")\n",
    "\n",
    "enhanced_records = []\n",
    "for site in sites[:25]:  # Windows-optimized subset\n",
    "    rssi = round(random.uniform(-85, -65), 2)\n",
    "    \n",
    "    # Calculate signal quality category\n",
    "    if rssi > -70:\n",
    "        quality = \"Excellent\"\n",
    "    elif rssi > -80:\n",
    "        quality = \"Good\"\n",
    "    else:\n",
    "        quality = \"Poor\"\n",
    "    \n",
    "    # Calculate Windows compatibility score\n",
    "    base_score = 85 if site[\"technology\"] == \"8G\" else (75 if site[\"technology\"] == \"7G\" else 65)\n",
    "    if site[\"vendor\"] == \"Samsong\":\n",
    "        base_score += 10  # Better Windows integration\n",
    "    elif site[\"vendor\"] == \"Noson\":\n",
    "        base_score += 5\n",
    "    \n",
    "    windows_score = min(100, max(0, base_score + random.uniform(-10, 10)))\n",
    "    \n",
    "    record = {\n",
    "        \"timestamp\": datetime.now() + timedelta(minutes=random.randint(61, 120)),\n",
    "        \"region\": site[\"region\"],\n",
    "        \"city\": site[\"city\"],\n",
    "        \"site_id\": site[\"site_id\"],\n",
    "        \"technology\": site[\"technology\"],\n",
    "        \"vendor\": site[\"vendor\"],\n",
    "        \"rssi_dbm\": rssi,\n",
    "        \"latency_ms\": round(random.uniform(25, 75), 2),\n",
    "        \"data_volume_mb\": round(random.uniform(1, 15), 2),\n",
    "        \"drop_rate_percent\": round(random.uniform(0, 2), 2),\n",
    "        \"cpu_usage_percent\": round(random.uniform(30, 80), 2),\n",
    "        \"signal_quality_category\": quality,\n",
    "        \"windows_compatibility_score\": round(windows_score, 2)\n",
    "    }\n",
    "    enhanced_records.append(record)\n",
    "\n",
    "# Insert enhanced data\n",
    "enhanced_df = spark.createDataFrame(enhanced_records)\n",
    "enhanced_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .saveAsTable(\"local.db.telecom_metrics\")\n",
    "\n",
    "print(\"✅ Enhanced data inserted\")\n",
    "\n",
    "# Query with mixed schema data\n",
    "print(\"\\n🔍 Querying mixed schema data:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(signal_quality_category) as records_with_quality,\n",
    "        COUNT(windows_compatibility_score) as records_with_windows_score,\n",
    "        ROUND(AVG(windows_compatibility_score), 2) as avg_windows_score\n",
    "    FROM local.db.telecom_metrics\n",
    "    GROUP BY region\n",
    "    ORDER BY region\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"✅ Schema evolution completed successfully on Windows!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Advanced Analytics & Windows Best Practices\n",
    "\n",
    "Enterprise-grade analytics and Windows-specific deployment recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analytics & Windows Best Practices\n",
    "print(\"📊 Advanced Analytics & Windows Best Practices\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Performance optimization queries\n",
    "print(\"\\n🚀 Performance Optimization Analysis:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        HOUR(timestamp) as hour,\n",
    "        COUNT(*) as measurements,\n",
    "        ROUND(AVG(rssi_dbm), 2) as avg_rssi,\n",
    "        ROUND(AVG(latency_ms), 2) as avg_latency,\n",
    "        ROUND(AVG(cpu_usage_percent), 2) as avg_cpu\n",
    "    FROM local.db.telecom_metrics\n",
    "    GROUP BY HOUR(timestamp)\n",
    "    ORDER BY hour\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "# Windows-specific vendor analysis\n",
    "print(\"\\n🪟 Windows Vendor Compatibility Analysis:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        vendor,\n",
    "        technology,\n",
    "        COUNT(*) as sites,\n",
    "        ROUND(AVG(windows_compatibility_score), 2) as avg_windows_score,\n",
    "        ROUND(AVG(rssi_dbm), 2) as avg_rssi\n",
    "    FROM local.db.telecom_metrics\n",
    "    WHERE windows_compatibility_score IS NOT NULL\n",
    "    GROUP BY vendor, technology\n",
    "    ORDER BY avg_windows_score DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Data compaction for Windows\n",
    "print(\"\\n🗜️ Data Compaction (Windows Optimization):\")\n",
    "print(\"   Compacting data files for better Windows performance...\")\n",
    "try:\n",
    "    spark.sql(\"CALL local.system.rewrite_data_files('db.telecom_metrics')\")\n",
    "    print(\"   ✅ Data compaction completed\")\n",
    "except:\n",
    "    print(\"   ℹ️ Compaction not available in this Iceberg version\")\n",
    "\n",
    "# Table statistics\n",
    "print(\"\\n📈 Table Statistics:\")\n",
    "try:\n",
    "    files_info = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_files,\n",
    "            ROUND(SUM(file_size_in_bytes) / 1024 / 1024, 2) as total_size_mb\n",
    "        FROM local.db.telecom_metrics.files\n",
    "    \"\"\")\n",
    "    files_info.show()\n",
    "except:\n",
    "    print(\"   ℹ️ File statistics not available\")\n",
    "\n",
    "print(\"\\n💡 Windows Best Practices Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"✅ Windows Environment:\")\n",
    "print(\"   • Use SSD storage for warehouse directories\")\n",
    "print(\"   • Configure Windows Defender exclusions for Spark/Java\")\n",
    "print(\"   • Set appropriate JVM heap sizes for Windows memory\")\n",
    "print(\"   • Use Windows file system monitoring for data integrity\")\n",
    "\n",
    "print(\"\\n✅ Performance Optimization:\")\n",
    "print(\"   • Enable adaptive query execution\")\n",
    "print(\"   • Use columnar file formats (Parquet)\")\n",
    "print(\"   • Regular compaction for optimal read performance\")\n",
    "print(\"   • Monitor query plans and execution times\")\n",
    "\n",
    "print(\"\\n✅ Security & Compliance:\")\n",
    "print(\"   • Integrate with Windows Active Directory\")\n",
    "print(\"   • Use Windows file permissions for access control\")\n",
    "print(\"   • Enable audit logging for compliance\")\n",
    "print(\"   • Implement encryption at rest and in transit\")\n",
    "\n",
    "print(\"\\n🚀 Windows deployment completed successfully!\")\n",
    "print(\"📊 Your Iceberg telecom data lake is ready for enterprise use!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark Configuration with Iceberg\n",
    "\n",
    "Configure Spark to work with Apache Iceberg. In enterprise environments, you would typically configure this in your cluster settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum, avg, count\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Windows-specific paths (using forward slashes for consistency)\n",
    "jar_url = \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar\"\n",
    "jar_path = \"./iceberg-spark-runtime.jar\"\n",
    "warehouse_path = \"./iceberg-warehouse\"\n",
    "\n",
    "# S3 configuration (commented out - uncomment and configure for S3)\n",
    "# s3_warehouse_path = \"s3a://your-iceberg-bucket/your-warehouse-path\"\n",
    "# s3_access_key = \"YOUR_AWS_ACCESS_KEY_ID\"\n",
    "# s3_secret_key = \"YOUR_AWS_SECRET_ACCESS_KEY\"\n",
    "# s3_hadoop_jar = \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar\"\n",
    "# s3_bundle_jar = \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar\"\n",
    "\n",
    "# Create warehouse directory if it doesn't exist (Windows-compatible)\n",
    "os.makedirs(warehouse_path, exist_ok=True)\n",
    "\n",
    "# Download Iceberg JAR for Windows (using requests for better Windows compatibility)\n",
    "print(\"📥 Downloading Iceberg JAR for Windows...\")\n",
    "try:\n",
    "    # Use requests instead of wget (which may not be available on Windows)\n",
    "    response = requests.get(jar_url, timeout=300)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(jar_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    print(f\"✅ Downloaded JAR to: {os.path.abspath(jar_path)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to download JAR: {e}\")\n",
    "    print(\"💡 Check your internet connection and try again\")\n",
    "    print(\"💡 You may need to download manually from:\")\n",
    "    print(f\"   {jar_url}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Download S3 related JARs if using S3 (uncomment below if using S3)\n",
    "# if 's3_hadoop_jar' in locals():\n",
    "#     print(\"📥 Downloading Hadoop AWS JAR for Windows...\")\n",
    "#     try:\n",
    "#         s3_response = requests.get(s3_hadoop_jar, timeout=300)\n",
    "#         s3_response.raise_for_status()\n",
    "#         with open(\"./hadoop-aws.jar\", 'wb') as f:\n",
    "#             f.write(s3_response.content)\n",
    "#         print(\"✅ Downloaded Hadoop AWS JAR\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to download Hadoop AWS JAR: {e}\")\n",
    "# \n",
    "# if 's3_bundle_jar' in locals():\n",
    "#     print(\"📥 Downloading AWS SDK Bundle JAR for Windows...\")\n",
    "#     try:\n",
    "#         bundle_response = requests.get(s3_bundle_jar, timeout=300)\n",
    "#         bundle_response.raise_for_status()\n",
    "#         with open(\"./aws-java-sdk-bundle.jar\", 'wb') as f:\n",
    "#             f.write(bundle_response.content)\n",
    "#         print(\"✅ Downloaded AWS SDK Bundle JAR\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to download AWS SDK Bundle JAR: {e}\")\n",
    "\n",
    "# Configure Spark with Iceberg for Windows\n",
    "print(\"\\n⚡ Initializing Spark with Iceberg on Windows...\")\n",
    "\n",
    "# Windows-specific Spark configuration\n",
    "spark_builder = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Telecom Demo - Windows\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "# Configure for local warehouse\n",
    "spark_builder = spark_builder.config(\"spark.sql.catalog.local.warehouse\", warehouse_path) \\\n",
    "                             .config(\"spark.sql.warehouse.dir\", warehouse_path)\n",
    "\n",
    "# Configure for S3 warehouse (commented out - uncomment and configure for S3)\n",
    "# spark_builder = spark_builder.config(\"spark.sql.catalog.local.warehouse\", s3_warehouse_path) \\\n",
    "#                              .config(\"spark.sql.warehouse.dir\", s3_warehouse_path) \\\n",
    "#                              .config(\"spark.jars\", f\"{jar_path},./hadoop-aws.jar,./aws-java-sdk-bundle.jar\") \\\n",
    "#                              .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "#                              .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "#                              .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# Windows-specific: Handle potential firewall/antivirus issues\n",
    "spark_builder = spark_builder.config(\"spark.driver.host\", \"localhost\")\n",
    "spark_builder = spark_builder.config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "\n",
    "try:\n",
    "    spark = spark_builder.getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce noise\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    print(f\"✅ Spark {spark.version} with Iceberg initialized on Windows!\")\n",
    "    print(f\"📁 Warehouse: {os.path.abspath(warehouse_path)}\")\n",
    "    print(f\"☕ Java Home: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "    print(f\"🖥️ Spark UI: http://localhost:4040\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize Spark: {e}\")\n",
    "    print(\"\\n💡 Windows troubleshooting:\")\n",
    "    print(\"   1. Check Windows Defender/Antivirus settings\")\n",
    "    print(\"   2. Ensure Java is properly installed\")\n",
    "    print(\"   3. Try running as Administrator\")\n",
    "    print(\"   4. Check firewall settings for Java/Python\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Synthetic Telecom Time Series Data\n",
    "\n",
    "Let's generate realistic telecom network performance data that represents typical enterprise telecom monitoring scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Telecom data generation for Windows\n",
    "print(\"📡 Generating Telecom Data for Windows...\")\n",
    "\n",
    "# Windows-optimized configuration\n",
    "chunk_size_seconds = 60\n",
    "site_count = 150  # Moderate size for Windows\n",
    "start_time = datetime(2023, 1, 1)\n",
    "demo_chunks = 75  # Good balance for Windows performance\n",
    "\n",
    "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "cities = {\n",
    "    \"North\": [\"Dendam\", \"Rondburg\", \"Nordville\"],\n",
    "    \"South\": [\"Schieveste\", \"Southpark\"],\n",
    "    \"East\": [\"Schipstad\", \"Dort\", \"Eastport\"],\n",
    "    \"West\": [\"Damstad\", \"Westfield\"],\n",
    "    \"Central\": [\"Centrum\", \"Midtown\"]\n",
    "}\n",
    "technologies = [\"6G\", \"7G\", \"8G\"]\n",
    "vendors = [\"Ericia\", \"Noson\", \"Weihu\", \"Samsong\"]\n",
    "\n",
    "# Generate site metadata\n",
    "print(\"🏗️ Creating site infrastructure...\")\n",
    "sites = []\n",
    "for i in range(site_count):\n",
    "    region = random.choice(regions)\n",
    "    city = random.choice(cities[region])\n",
    "    tech = random.choices(technologies, weights=[0.3, 0.5, 0.2])[0]\n",
    "    vendor = random.choices(vendors, weights=[0.3, 0.3, 0.2, 0.2])[0]\n",
    "    site_id = f\"SITE_{i:05d}\"\n",
    "    sites.append({\n",
    "        \"site_id\": site_id,\n",
    "        \"region\": region,\n",
    "        \"city\": city,\n",
    "        \"technology\": tech,\n",
    "        \"vendor\": vendor,\n",
    "    })\n",
    "\n",
    "def generate_telecom_data_chunk(second_offset):\n",
    "    \"\"\"Generate telecom metrics for a specific time offset\"\"\"\n",
    "    timestamp = start_time + timedelta(seconds=second_offset)\n",
    "    records = []\n",
    "    \n",
    "    for site in sites:\n",
    "        region = site[\"region\"]\n",
    "        city = site[\"city\"]\n",
    "        tech = site[\"technology\"]\n",
    "        vendor = site[\"vendor\"]\n",
    "\n",
    "        # Enhanced signal strength (RSSI) modeling\n",
    "        base_rssi = -70 if tech == \"8G\" else (-75 if tech == \"7G\" else -85)\n",
    "        if vendor == \"Noson\":\n",
    "            base_rssi += 3\n",
    "        elif vendor == \"Weihu\":\n",
    "            base_rssi -= 2\n",
    "        elif vendor == \"Samsong\":\n",
    "            base_rssi += 1\n",
    "        \n",
    "        # Add regional variations\n",
    "        if region == \"Central\":\n",
    "            base_rssi += 2  # Better infrastructure in central areas\n",
    "        \n",
    "        rssi = np.random.normal(loc=base_rssi, scale=3)\n",
    "\n",
    "        # Latency modeling with Windows-specific considerations\n",
    "        if region == \"Central\":\n",
    "            base_latency = 25 if tech == \"8G\" else (35 if tech == \"7G\" else 60)\n",
    "        else:\n",
    "            base_latency = 30 if tech == \"8G\" else (40 if tech == \"7G\" else 65)\n",
    "        \n",
    "        latency = np.random.normal(loc=base_latency, scale=8)\n",
    "\n",
    "        # Data volume (in MB)\n",
    "        data_volume = np.random.exponential(scale=7)\n",
    "\n",
    "        # CPU usage with vendor-specific characteristics\n",
    "        base_cpu = 45 + (5 if tech == \"8G\" else (7 if tech == \"7G\" else 0))\n",
    "        if vendor == \"Weihu\":\n",
    "            base_cpu += 8\n",
    "        elif vendor == \"Samsong\":\n",
    "            base_cpu += 3\n",
    "        cpu_usage = np.clip(np.random.normal(loc=base_cpu, scale=10), 0, 100)\n",
    "\n",
    "        # Drop rate calculation (using built-in min function)\n",
    "        city_penalty = 0.03 if city in [\"Damstad\", \"Schieveste\"] else 0.008\n",
    "        vendor_penalty = 0.015 if vendor == \"Weihu\" else 0.005\n",
    "        drop_rate = (\n",
    "            min(1.0, 0.001 * cpu_usage + city_penalty + vendor_penalty + np.random.beta(1, 200)) * 100\n",
    "        )\n",
    "\n",
    "        records.append({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"region\": region,\n",
    "            \"city\": city,\n",
    "            \"site_id\": site[\"site_id\"],\n",
    "            \"technology\": tech,\n",
    "            \"vendor\": vendor,\n",
    "            \"rssi_dbm\": round(rssi, 2),\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"data_volume_mb\": round(data_volume, 2),\n",
    "            \"drop_rate_percent\": round(drop_rate, 2),\n",
    "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
    "        })\n",
    "    return records\n",
    "\n",
    "# Generate telecom time series data with Windows progress indication\n",
    "print(\"⚡ Generating comprehensive telecom metrics for Windows...\")\n",
    "print(\"📊 Progress: \", end=\"\", flush=True)\n",
    "\n",
    "all_records = []\n",
    "progress_step = demo_chunks // 10\n",
    "\n",
    "for chunk_idx in range(demo_chunks):\n",
    "    chunk_records = []\n",
    "    for second in range(chunk_size_seconds):\n",
    "        minute_offset = chunk_idx * chunk_size_seconds + second\n",
    "        chunk_records.extend(generate_telecom_data_chunk(minute_offset))\n",
    "    all_records.extend(chunk_records)\n",
    "    \n",
    "    # Show progress\n",
    "    if chunk_idx % progress_step == 0:\n",
    "        print(\"█\", end=\"\", flush=True)\n",
    "\n",
    "print(\" ✅\")\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "print(\"🔄 Converting to Spark DataFrames...\")\n",
    "telecom_metrics_df = spark.createDataFrame(all_records)\n",
    "sites_df = spark.createDataFrame(sites)\n",
    "\n",
    "print(\"✅ Windows telecom data generated successfully!\")\n",
    "print(f\"   📡 Sites: {sites_df.count():,}\")\n",
    "print(f\"   📊 Metrics: {telecom_metrics_df.count():,}\")\n",
    "print(f\"   💾 Memory usage: ~{len(all_records) * 200 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\n📊 Sample Telecom Data:\")\n",
    "telecom_metrics_df.show(10)\n",
    "print(\"\\n🏢 Sample Site Data:\")\n",
    "sites_df.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
